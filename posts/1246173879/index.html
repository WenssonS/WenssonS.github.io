

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/avatar_img.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="WenssonS">
  <meta name="keywords" content="">
  
    <meta name="description" content="0 写在前面推荐系统是一种信息过滤系统，通过预测用户对物品的评分或偏好，来推荐用户未交互过的物品。推荐的对象包括但不限于：视频、音乐、新闻、书籍、电商平台中的各类商品等。 在过去的二三十年里，伴随着互联网的快速兴起，海量的信息和数据呈现爆发式的增长，这被人们称为信息超载（Information Overload）。为应对信息超载问题，推荐系统得到了十分迅猛的发展，衍生出了非常多的模型和方法。由于推">
<meta property="og:type" content="article">
<meta property="og:title" content="推荐之传统模型">
<meta property="og:url" content="http://example.com/posts/1246173879/index.html">
<meta property="og:site_name" content="WenssonS&#39;s Zone">
<meta property="og:description" content="0 写在前面推荐系统是一种信息过滤系统，通过预测用户对物品的评分或偏好，来推荐用户未交互过的物品。推荐的对象包括但不限于：视频、音乐、新闻、书籍、电商平台中的各类商品等。 在过去的二三十年里，伴随着互联网的快速兴起，海量的信息和数据呈现爆发式的增长，这被人们称为信息超载（Information Overload）。为应对信息超载问题，推荐系统得到了十分迅猛的发展，衍生出了非常多的模型和方法。由于推">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/traditional_rec_classification.jpg">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/traditional_rec_evolution.jpg">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/osjl.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/cxsd.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/pexxsd.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/jxsd.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/svd.jpg">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/funksvd.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/funksvd_loss.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/biassvd.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/biassvd_loss.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/svd++.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/timesvd.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/lr.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/sigmoid.jpg">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/cross_entropy_loss.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/poly2.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/mlr2.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/mlr.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/fm.png">
<meta property="og:image" content="http://example.com/img/post/rec_traditional/ffm.png">
<meta property="article:published_time" content="2022-04-18T04:39:33.000Z">
<meta property="article:modified_time" content="2022-06-29T14:12:43.477Z">
<meta property="article:author" content="WenssonS">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="recommendation">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/post/rec_traditional/traditional_rec_classification.jpg">
  
  
  <title>推荐之传统模型 - WenssonS&#39;s Zone</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":false,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":4},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="WenssonS's Zone" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>WenssonS&#39;s Zone</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archive
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Category
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tag
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/wallpaper.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="推荐之传统模型"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-04-18 12:39" pubdate>
          2022年4月18日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    

    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar category-bar" style="margin-right: -1rem">
    





<div class="category-list">
  
  
    
    
    
    <div class="category row nomargin-x">
      <a class="category-item 
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="recommendation"
        id="heading-5ba88d25175d87cb5977cb56e01ed17c" role="tab" data-toggle="collapse" href="#collapse-5ba88d25175d87cb5977cb56e01ed17c"
        aria-expanded="true"
      >
        recommendation
        <span class="list-group-count">(3)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse show" id="collapse-5ba88d25175d87cb5977cb56e01ed17c"
           role="tabpanel" aria-labelledby="heading-5ba88d25175d87cb5977cb56e01ed17c">
        
        
          
  <div class="category-post-list">
    
    
      
      
        <a href="/posts/26532/" title="推荐之Embedding技术"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">推荐之Embedding技术</span>
        </a>
      
    
      
      
        <a href="/posts/1246173879/" title="推荐之传统模型"
           class="list-group-item list-group-item-action
           active">
          <span class="category-post">推荐之传统模型</span>
        </a>
      
    
      
      
        <a href="/posts/2110556878/" title="推荐之框架理解"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">推荐之框架理解</span>
        </a>
      
    
  </div>

        
      </div>
    </div>
  
</div>


  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">推荐之传统模型</h1>
            
            <div class="markdown-body">
              
              <h1 id="0-写在前面"><a href="#0-写在前面" class="headerlink" title="0 写在前面"></a>0 写在前面</h1><p><strong>推荐系统</strong>是一种信息过滤系统，通过预测用户对物品的评分或偏好，来推荐用户未交互过的物品。推荐的对象包括但不限于：视频、音乐、新闻、书籍、电商平台中的各类商品等。</p>
<p>在过去的二三十年里，伴随着互联网的快速兴起，海量的信息和数据呈现爆发式的增长，这被人们称为<strong>信息超载（Information Overload）</strong>。为应对信息超载问题，推荐系统得到了十分迅猛的发展，衍生出了非常多的模型和方法。由于推荐系统本身可以从多个角度进行衡量和评价，并且多种推荐算法之间往往存在演进与结合的关系，其分界线并不十分清晰，因此对其进行系统而统一的梳理分类是一件比较头疼的事情。</p>
<p><strong>该post致力于整理深度学习大行其道之前的这段时间里，推荐系统领域的主流方法或模型，并将与接下来几个post共同整理推荐系统领域的一些常用算法模型和问题解法。由于推荐系统领域的复杂和多样，区区几篇post是不可能整理完所有内容的，因此将按我所了解及感兴趣的思路来进行整理。</strong></p>
<h1 id="1-传统推荐算法的分类"><a href="#1-传统推荐算法的分类" class="headerlink" title="1 传统推荐算法的分类"></a>1 传统推荐算法的分类</h1><p>传统的推荐方法一般可分为<strong>基于内容（Content-based）的推荐</strong>、<strong>基于协同过滤（Collaborative Filtering）的推荐</strong>及<strong>混合式（Hybrid）的推荐</strong>。<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions
">[1]</span></a></sup> 至于基于规则（Rule-based）或者基于人口统计信息（Demographic-based）等其他较为小众的类别，本文不作介绍。下图展示了大概的分类框架：</p>
<p><img src="/img/post/rec_traditional/traditional_rec_classification.jpg" srcset="/img/loading.gif" lazyload alt="传统推荐算法分类"></p>
<h2 id="1-1-基于内容的推荐"><a href="#1-1-基于内容的推荐" class="headerlink" title="1.1 基于内容的推荐"></a>1.1 基于内容的推荐</h2><p>顾名思义，该方法专注于通过分析物品的内容或属性，找到一个用户过去所交互过的物品的共性，来找到该用户可能感兴趣的物品。这类方法起源于检索领域，因此主要考虑文本信息，比如文档、URL、TF-IDF等。用户的偏好即通过其所交互过的物品的内容信息聚合而来，通过用户偏好与物品的内容信息来对未交互过的物品进行分数预测。当然，在具体计算的过程中，也有些方法使用如朴素贝叶斯、决策树、神经网络等整合内容信息，来预测用户对一个物品的分数。</p>
<p>该方法忽略用户行为，只考虑物品与物品的相关性，因此不太受物品冷启动问题的影响。并且整个过程简单，可离线进行，因此推荐响应速度快，简洁有效。此外，由于每个用户和物品都由内容属性来表示，因此具有比较好的可解释性。</p>
<p>该方法只能对文本进行较为有效的分析，除了文本，诸如图像、音频、视频等内容信息都不太好提取其特征。并且即使在处理文本时，特征的提取也需要仔细地设计，比如使用关键词信息表示物品，容易出现两个物品的内容信息一致的情况，进而无法对其进行有效区分。特征抽取方面的困难，限制了该方法的上限。此外，该方法的推荐多样性不够好，这会使推荐的物品只与用户所交互过的物品具有强相关性，而其他的物品则由于内容差别过大而不会被推荐。当然，除了上述问题，该方法也深受用户冷启动问题的困扰。</p>
<h2 id="1-2-基于协同过滤的推荐"><a href="#1-2-基于协同过滤的推荐" class="headerlink" title="1.2 基于协同过滤的推荐"></a>1.2 基于协同过滤的推荐</h2><p>协同过滤本质上是一大类推荐算法，目前比较主流的看法是将基于协同过滤的推荐进一步细分为<strong>基于存量（Memory-based）的协同过滤</strong>和<strong>基于模型（Model-based）的协同过滤</strong>，而基于存量（Memory-based）的协同过滤则进一步细分为大家所熟知的<strong>基于用户（User-based）的协同过滤</strong>和基于<strong>物品（Item-based）的协同过滤</strong>。<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="协同过滤维基百科
">[2]</span></a></sup></p>
<p>一般所讲的协同过滤专指基于存量的协同过滤，即UserCF和ItemCF，这可以算是狭义上的协同过滤。而基于模型的协同过滤算法，主要是通过用户-物品评分矩阵训练一个机器学习模型，然后输入用户和物品ID得到评分预测。因此在讨论协同过滤的时候，很容易陷入一个误区，即以为只有UserCF与ItemCF两种协同过滤方法。然而由于协同过滤的定义十分宽泛，因此事实上几乎所有的推荐算法，比如早期的矩阵分解、逻辑回归，以及现有的各类神经网络模型，都可以被看作是基于模型的协同过滤的变种或改进算法，这种体会会随着对推荐算法的了解而逐渐加深。本文只讨论传统的推荐模型，为了让讨论更有章可循，<strong>在该post的以下内容中，我们只讨论狭义上的协同过滤，所讲的协同过滤仅指代基于存量的协同过滤，即UserCF与ItemCF。</strong></p>
<p>协同过滤算法简单有效，工程上容易实现，不需要过多领域知识，与物品本身的内容属性无关，无需考虑对内容的特征抽取过程。但是该方法受数据稀疏、冷启动问题的困扰，模型泛化能力不足，并且不能够引入其他对推荐有用的特征。</p>
<h2 id="1-3-混合式的推荐"><a href="#1-3-混合式的推荐" class="headerlink" title="1.3 混合式的推荐"></a>1.3 混合式的推荐</h2><p>与其说混合式推荐是一种分类，不如说现实中的推荐系统大多都是这样设计的。其思想即为组合多种推荐方法，将它们的优点结合起来进行互补，以达到更好的推荐效果，具体方法这里不再赘述。</p>
<h1 id="2-传统推荐算法的演进"><a href="#2-传统推荐算法的演进" class="headerlink" title="2 传统推荐算法的演进"></a>2 传统推荐算法的演进</h1><p>在讨论完传统推荐算法的分类后，接下来会按照算法模型的演进过程，讨论一些经典推荐算法的细节问题。传统推荐算法以协同过滤为出发点，衍生出了两大类经典方法，一类着力于提升协同过滤的学习能力和泛化能力，发展出了<strong>矩阵分解（Matrix Factorization，简称MF）</strong>方法；一类研究如何更好地引入更多的特征，发展出了<strong>逻辑回归（Logistic Regression，简称LR）</strong>方法。后续方法，包括现今各类的神经网络模型，也是基于这两条路进行模型层面的改进。下图借鉴了王喆的《深度学习推荐系统》的框架图，展示了该演化过程：</p>
<p><img src="/img/post/rec_traditional/traditional_rec_evolution.jpg" srcset="/img/loading.gif" lazyload alt="传统推荐算法演进"></p>
<h2 id="2-1-协同过滤"><a href="#2-1-协同过滤" class="headerlink" title="2.1 协同过滤"></a>2.1 协同过滤</h2><p>如前文所述，这里讨论UserCF与ItemCF，是由亚马逊于2003年提出的两个最经典的协同过滤算法。</p>
<h3 id="2-1-1-基于用户和物品的协同过滤"><a href="#2-1-1-基于用户和物品的协同过滤" class="headerlink" title="2.1.1 基于用户和物品的协同过滤"></a>2.1.1 基于用户和物品的协同过滤</h3><p>两种协同过滤算法本质上是一样的，并且都需要用到用户-物品评分矩阵，区别在于两者分别从用户和物品的角度进行计算。评分矩阵中，每一行为一个用户向量，每一列为一个物品向量。</p>
<p>UserCF过程如下：</p>
<ul>
<li>根据用户交互记录，计算用户相似度</li>
<li>找到与目标用户最相似的K个用户</li>
<li>在这K个用户所喜欢或交互过的物品中，找到目标用户未交互过的进行推荐</li>
</ul>
<p> ItemCF过程如下：</p>
<ul>
<li>根据物品交互记录，计算物品相似度</li>
<li>找到与目标物品最相似的K个物品</li>
<li>在这K个物品中，找到与这些物品交互过且未与目标物品交互过的用户，将目标物品推荐给这些用户</li>
</ul>
<h3 id="2-1-2-相似度计算公式"><a href="#2-1-2-相似度计算公式" class="headerlink" title="2.1.2 相似度计算公式"></a>2.1.2 相似度计算公式</h3><p>协同过滤的核心在于如何更好地计算相似度，如下给出几种常用方法。</p>
<h4 id="2-1-2-1-闵可夫斯基距离"><a href="#2-1-2-1-闵可夫斯基距离" class="headerlink" title="2.1.2.1 闵可夫斯基距离"></a>2.1.2.1 闵可夫斯基距离</h4><p>闵可夫斯基距离（Minkowski Distance）其实是一类距离的统称，其计算方式与向量的范数计算本质上是一样的：<br><img src="/img/post/rec_traditional/osjl.png" srcset="/img/loading.gif" lazyload></p>
<p>当p&#x3D;1时，计算的便是曼哈顿距离（Manhattan Distance）。<br>当p&#x3D;2时，计算的便是<strong>欧氏距离（Euclidean Distance）</strong>。欧式距离为最常见的相似度计算方式，易于理解和实现，但是只适用于欧式空间。不过在协同过滤的场景中，已经能够满足需求。<br>当p趋于无穷时，计算的便是切比雪夫距离（Chebyshev Distance）。</p>
<p>欧式距离默认每个维度的权重都是相同的，因此当某个维度的值非常大时，非常容易由该维度主导整个计算结果。因此在计算欧氏距离之前，最好是先对数据做归一化处理。</p>
<h4 id="2-1-2-2-余弦相似度"><a href="#2-1-2-2-余弦相似度" class="headerlink" title="2.1.2.2 余弦相似度"></a>2.1.2.2 余弦相似度</h4><p><strong>余弦相似度（Cosine Similarity）</strong>取值区间在[-1, 1]之间，越靠近1表明两个向量相似度越高<br><img src="/img/post/rec_traditional/cxsd.png" srcset="/img/loading.gif" lazyload></p>
<p>相比于欧氏距离，余弦相似度计算的是一种相对的距离，与向量的幅度无关，只与其方向有关。在关注绝对差异时，比如用户的活跃度时，则欧氏距离能够比较好地衡量活跃度差异。但是在关注相对差异时，比如两个用户的物品交互记录分别为[3, 5, 3]和[4, 6, 4]，他们的兴趣应该是差不多相似的，但是欧氏距离并不能很好地计算他们的相似度。在推荐场景中，用户和物品常常存在<strong>偏置问题（bias）</strong>，余弦相似度能够起到一些<strong>去偏（debias）</strong>的作用。</p>
<p>余弦相似度计算比较直接，容易受到维度缺失的影响。而在实际的推荐场景中，不可避免地会出现某些维度的值为null的情况。</p>
<h4 id="2-1-2-3-皮尔逊相似度"><a href="#2-1-2-3-皮尔逊相似度" class="headerlink" title="2.1.2.3 皮尔逊相似度"></a>2.1.2.3 皮尔逊相似度</h4><p>在余弦相似度基础上，每个向量减去这个向量均值组成的向量，便是<strong>皮尔逊相似度（Pearson Correlation Similarity）</strong>，也称皮尔逊相关系数。<br><img src="/img/post/rec_traditional/pexxsd.png" srcset="/img/loading.gif" lazyload></p>
<p>相比于欧氏距离，皮尔逊相似度对不同变量的取值范围没有要求，变量的量纲差别在计算过程中被去掉了，等价于z-score标准化。</p>
<p>相比于余弦相似度，皮尔逊相似度相当于在计算余弦相似度之前，先做了中心化的操作。在中心化时，默认会将所有缺失的维度填上0值，因此能够解决余弦相似度解决不了的维度缺失问题。当两个向量均值为0时，皮尔逊相似度即余弦相似度。在推荐场景中，中心化操作也使皮尔逊相似度能够更好地起到去偏的作用。</p>
<h4 id="2-1-2-4-Jaccard相似度"><a href="#2-1-2-4-Jaccard相似度" class="headerlink" title="2.1.2.4 Jaccard相似度"></a>2.1.2.4 Jaccard相似度</h4><p><strong>Jaccard相似度（Jaccard Similarity）</strong>是衡量两个集合相似度的方法，也可以用于协同过滤场景。<br><img src="/img/post/rec_traditional/jxsd.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="2-1-3-两种协同过滤的区别"><a href="#2-1-3-两种协同过滤的区别" class="headerlink" title="2.1.3 两种协同过滤的区别"></a>2.1.3 两种协同过滤的区别</h3><p>虽然两者过程相似，但是在工业界的实际使用中两者区别还是蛮大的，ItemCF的使用要明显多于UserCF。</p>
<p>UserCF：</p>
<ul>
<li>用户数较少的场景，复杂度O(M<sup>2</sup>N)，M为用户数，N为物品数</li>
<li>推荐偏热门，多样性较差</li>
<li>时效性强，用户个性化需求不太明显，推荐粒度较粗</li>
<li>可解释性不好</li>
<li>新闻、社交领域等</li>
</ul>
<p>ItemCF：</p>
<ul>
<li>物品数较少的场景，复杂度O(N<sup>2</sup>M)，M为用户数，N为物品数</li>
<li>推荐偏个性化，多样性较好</li>
<li>用户兴趣比较固定，个性化需求强，推荐粒度较细</li>
<li>可解释性较好</li>
<li>电商、电影、书籍等</li>
</ul>
<h3 id="2-1-4-优缺点"><a href="#2-1-4-优缺点" class="headerlink" title="2.1.4 优缺点"></a>2.1.4 优缺点</h3><p>优点：</p>
<ul>
<li>简单有效，易于实现</li>
<li>可离线生成计算结果，容易上线</li>
</ul>
<p>缺点：</p>
<ul>
<li>受冷启动问题的制约，对冷启动用户和物品的推荐效果不好</li>
<li>头部效应明显，处理稀疏向量的能力比较弱，泛化能力比较差</li>
<li>仅考虑历史交互信息，无法引入用户和物品的属性、上下文等很多对推荐有用的信息，可扩展性不好</li>
</ul>
<h2 id="2-2-矩阵分解"><a href="#2-2-矩阵分解" class="headerlink" title="2.2 矩阵分解"></a>2.2 矩阵分解</h2><p>矩阵分解由Netflix于2006年提出。矩阵分解在协同过滤的基础上，加入了<strong>隐向量（Latent Vector）</strong>的概念来表示用户和物品，这加强了模型处理稀疏矩阵的能力，同时也增加了模型的学习能力与泛化能力。矩阵分解的提出同样意义非凡，如今几乎每一个推荐模型都会为用户和物品学习一套<strong>Embedding</strong>，即矩阵分解中的隐向量，用以表示它们的偏好特点。而用户对物品的预测分数，则一般使用两个Embedding的内积来表示。尽管在矩阵分解早期，推荐领域并没有Embedding的概念，但是从现今的角度回顾矩阵分解技术，将隐向量称为Embedding并不会引起歧义，因此为表述方便本小节统称为Embedding。</p>
<h3 id="2-2-1-经典方法"><a href="#2-2-1-经典方法" class="headerlink" title="2.2.1 经典方法"></a>2.2.1 经典方法</h3><h4 id="2-2-1-1-SVD"><a href="#2-2-1-1-SVD" class="headerlink" title="2.2.1.1 SVD"></a>2.2.1.1 SVD</h4><p>奇异值分解（Singular Value Decomposition，简称SVD），其公式如下：<br><img src="/img/post/rec_traditional/svd.jpg" srcset="/img/loading.gif" lazyload><br>分解完成后，矩阵$\mathbf{U}$与矩阵$\mathbf{V}$则分别表示用户和物品的Embedding矩阵。如果只取较大的<em>k</em>个奇异值，则两个矩阵中的Embedding维度也会被缩小至<em>k</em>。</p>
<p>虽然看起来SVD的理论能够完美套用在矩阵分解中，但是在实际场景中，SVD不可避免地会遇到如下两个问题：</p>
<ul>
<li>SVD的前提是矩阵是稠密的，然而实际评分矩阵非常稀疏。因此在使用SVD之前，必须对评分矩阵进行补全，如平均值等方法。但是补全又会带来两个问题，一个是增加算法复杂度，另一个是简单粗暴的补全会引入较为明显的误差。</li>
<li>传统的SVD的计算复杂度达到了O(MN<sup>2</sup>)，这种计算复杂度在实际推荐场景中是不可接受的。</li>
</ul>
<h4 id="2-2-1-2-FunkSVD"><a href="#2-2-1-2-FunkSVD" class="headerlink" title="2.2.1.2 FunkSVD"></a>2.2.1.2 FunkSVD</h4><p>FunkSVD是最经典的矩阵分解模型，也是最早的<strong>隐语义模型（Latent Factor Model）</strong>，它将评分矩阵直接分解为用户和物品两个Embedding矩阵，每个用户和物品通过其对应的Embedding来表示：<br><img src="/img/post/rec_traditional/funksvd.png" srcset="/img/loading.gif" lazyload><br>在具体分解矩阵的时候，FunkSVD采用<strong>梯度下降法（Gradient Descent）</strong>进行Embedding的优化，损失函数如下：<br><img src="/img/post/rec_traditional/funksvd_loss.png" srcset="/img/loading.gif" lazyload><br>FunkSVD使用用户和物品的Embedding内积作为预测得分，并且损失函数中还加入了L2正则化项防止过拟合。在FunkSVD的身上，我们已经能够看到现在的推荐算法的雏形了。</p>
<h4 id="2-2-1-3-BiasSVD"><a href="#2-2-1-3-BiasSVD" class="headerlink" title="2.2.1.3 BiasSVD"></a>2.2.1.3 BiasSVD</h4><p>BiasSVD主要是为了处理推荐场景中的偏置问题，在FunkSVD基础上，BiasSVD考虑如下三个bias部分：</p>
<ul>
<li>训练集中所有评分记录的平均数$\mu$</li>
<li>用户偏置$b_u$，表示用户$u$的打分习惯，有些用户可能倾向于打分高，有些用户可能倾向于打分低</li>
<li>物品偏置$b_i$，表示某一物品$i$得到的打分情况，与上一条相似</li>
</ul>
<p> 考虑偏置后的预测分数计算如下<br><img src="/img/post/rec_traditional/biassvd.png" srcset="/img/loading.gif" lazyload><br>考虑偏置后的损失函数如下：<br><img src="/img/post/rec_traditional/biassvd_loss.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="2-2-1-4-SVD"><a href="#2-2-1-4-SVD" class="headerlink" title="2.2.1.4 SVD++"></a>2.2.1.4 SVD++</h4><p>SVD++在BiasSVD的基础上又添加了用户的隐式反馈信息，如用户的浏览记录、收听记录等，相当于引入额外信息，帮助解决显式评分行为较少而导致的冷启动问题。</p>
<p>SVD++认为除了用户显式评分过的物品，有过行为的物品也都有Embedding表示。此外，用户的一些属性，比如社会统计学信息，转成01特征后，也都使其对应一个Embedding。这两者各自相加，表示一部分该用户的偏好：<br><img src="/img/post/rec_traditional/svd++.png" srcset="/img/loading.gif" lazyload><br>图中，$\mathbf{N}(u)$表示与用户$u$有过行为交互的物品集合，$\mathbf{A}(u)$表示用户$u$的属性集合。</p>
<h4 id="2-2-1-5-TimeSVD"><a href="#2-2-1-5-TimeSVD" class="headerlink" title="2.2.1.5 TimeSVD"></a>2.2.1.5 TimeSVD</h4><p>TimeSVD在BiasSVD基础上，加入了时间变化的影响，该模型认为物品的流行度以及用户的偏好都是会随时间流逝的， 计算公式如下：<br><img src="/img/post/rec_traditional/timesvd.png" srcset="/img/loading.gif" lazyload><br>其中$b_u(t)$、$b_i(t)$和$p_u(t)$分别是用户偏置、物品偏置和用户隐向量随时间变化的函数，TimeSVD不考虑物品隐向量的时间变化。在具体计算时间变化时，TimeSVD采用时间离散化或者高斯核来进行处理。</p>
<h3 id="2-2-2-优缺点"><a href="#2-2-2-优缺点" class="headerlink" title="2.2.2 优缺点"></a>2.2.2 优缺点</h3><p>优点：</p>
<ul>
<li>由于对每个用户和物品都抽象出Embedding来进行表示，因此泛化能力比协同过滤有所增强，并且对数据稀疏问题有一定缓解</li>
<li>空间复杂度显著降低，不再需要存储庞大的评分矩阵，只需要存储用户和物品的Embedding矩阵，由O(MN)降低到O((M+N)×k)</li>
<li>虽然前文一直在讲Embedding，但是这是为了方便表述和理解，请注意在以上几个方法提出来时，只有隐向量的概念，不过本质上矩阵分解就是为了产出Embedding，因此可以与现今的深度学习模型无缝衔接</li>
</ul>
<p> 缺点：</p>
<ul>
<li>以上几个经典的矩阵分解模型仍然没有考虑如何更好地引入用户、物品和各类上下文的有效信息，而SVD++也只是在这方面初步做出了尝试，矩阵分解系列算法的重心在于如何更好地根据评分矩阵分解出Embedding</li>
<li>由于基本上只考虑评分矩阵，因此无法对冷启动用户和物品进行有效推荐</li>
</ul>
<h2 id="2-3-逻辑回归"><a href="#2-3-逻辑回归" class="headerlink" title="2.3 逻辑回归"></a>2.3 逻辑回归</h2><p>逻辑回归是一个非常经典的机器学习模型，同时从深度学习的角度，也可以将其看作是一个单一神经元。相比于协同过滤和矩阵分解，逻辑回归将推荐问题看作一个分类问题，通过预测正样本的概率对物品进行排序，即将其转化为一个<strong>点击率（Click Through Rate，简称CTR）预估</strong>问题。</p>
<h3 id="2-3-1-数学形式"><a href="#2-3-1-数学形式" class="headerlink" title="2.3.1 数学形式"></a>2.3.1 数学形式</h3><p>逻辑回归本质上就是线性回归与Sigmoid函数的组合：<br><img src="/img/post/rec_traditional/lr.png" srcset="/img/loading.gif" lazyload><br>其中，$x$是特征向量，$w^T$与$b$是可训练的参数。在推荐场景中，各种各样的特征，包括离散特征和连续特征，拼接在一起所形成的一个大向量即为$x$。</p>
<p>Sigmoid函数阈值在(0, 1)区间内，两边均为开区间，函数如下：<br><img src="/img/post/rec_traditional/sigmoid.jpg" srcset="/img/loading.gif" lazyload><br>由于Sigmoid能将因变量映射到(0, 1)区间内，因此逻辑回归能够用来做二分类模型</p>
<h3 id="2-3-2-极大似然估计"><a href="#2-3-2-极大似然估计" class="headerlink" title="2.3.2 极大似然估计"></a>2.3.2 极大似然估计</h3><p>逻辑回归使用<strong>极大似然估计（Maximum Likelihood Estimate）</strong>的方法来训练各参数，极大似然估计是一种统计学中用来估计参数的方法。这里涉及到两个概念：<strong>概率</strong>和<strong>似然</strong>。概率，即已知规律，推导出某个事件理论上发生的可能性。似然，即已知结果，根据结果反推理论上各个事件的可能性分布。而极大似然估计即根据已知的样本信息，去反推出一套最有可能导致该结果的模型参数。这里省略逻辑回归中极大似然估计的推导过程，有兴趣可见该<a target="_blank" rel="noopener" href="https://blog.csdn.net/zjuPeco/article/details/77165974">博客</a>。</p>
<h3 id="2-3-3-损失函数选择"><a href="#2-3-3-损失函数选择" class="headerlink" title="2.3.3 损失函数选择"></a>2.3.3 损失函数选择</h3><p>这个问题在面试中一般也很常见，也是逻辑回归与矩阵分解的一个很大的不同。矩阵分解中一般用<strong>平方损失（Squared Loss）</strong>函数，但是逻辑回归并不使用平方损失函数。如果使用平方损失函数，则会由于Sigmoid函数的存在，导致损失函数为非凸函数，这会使逻辑回归模型陷入局部最优的陷阱中。</p>
<p>逻辑回归使用<strong>对数似然损失（Log-likehood Loss）</strong>函数或<strong>交叉熵损失（Cross Entropy Loss）</strong>函数。在二分类场景下，这两者的表达式其实是一样的。但是对数似然损失只适用于二分类场景，即模型最后一层是Sigmoid函数，而交叉熵损失可以拓展到多分类场景，即模型最后一层是Softmax函数。从极大似然估计的角度出发，推导出来的损失函数即交叉熵损失函数，并且该函数是一个凸函数，方便使用梯度下降法进行学习和优化。</p>
<p>逻辑回归中的交叉熵损失函数如下：<br><img src="/img/post/rec_traditional/cross_entropy_loss.png" srcset="/img/loading.gif" lazyload><br>在交叉熵损失函数基础上，逻辑回归使用梯度下降法来进行参数的更新优化。</p>
<h3 id="2-3-4-优缺点"><a href="#2-3-4-优缺点" class="headerlink" title="2.3.4 优缺点"></a>2.3.4 优缺点</h3><p>优点：</p>
<ul>
<li>逻辑回归假设因变量符合伯努利分布，而CTR问题的因变量也符合伯努利分布，因此在数学含义上是相符的</li>
<li>可解释性强，逻辑回归本质上是各特征的加权求和，因此查看各特征的权重，便可得知某物品被推荐的理由</li>
<li>模型简单有效，易于实现，训练开销小，而训练开销和在线推断又是工业界的一个痛点需求，在2012年GPU流行之前，逻辑回归都是工业界的主流模型</li>
</ul>
<p>缺点：</p>
<ul>
<li>性能极其依赖特征工程</li>
<li>逻辑回归本质上是个线性模型，结构简单，且不具备特征交叉能力</li>
</ul>
<h3 id="2-3-5-连续or离散"><a href="#2-3-5-连续or离散" class="headerlink" title="2.3.5 连续or离散"></a>2.3.5 连续or离散</h3><p>在工业界，一般很少将连续值直接输入到逻辑回归模型中，而是将其进行<strong>离散化</strong>，亦被称作<strong>特征分桶</strong>，转化成一系列0&#x2F;1的离散特征。这样做的优势在于：</p>
<ul>
<li>离散特征使模型可扩展性更好，添加和删除都十分方便，易于快速迭代</li>
<li>离散化后的特征鲁棒性很好，比如将大于30的年龄转化成1，否则是0，那么假如有一个异常数据300，如果直接作为连续值输入会给模型带来很大噪音</li>
<li>离散化后，相当于为模型在特征层面引入了非线性，能够提升模型的表达和拟合能力</li>
<li>离散化后的特征方便进行特征交叉，进一步提高模型的表达能力</li>
<li>许多连续特征，比如年龄，增减一岁区别不大，因此将其按区间离散化后可以使模型更稳定，同时也降低了模型过拟合的风险</li>
</ul>
<h3 id="2-3-6-逻辑回归的两个变种模型"><a href="#2-3-6-逻辑回归的两个变种模型" class="headerlink" title="2.3.6 逻辑回归的两个变种模型"></a>2.3.6 逻辑回归的两个变种模型</h3><h4 id="2-3-6-1-POLY2"><a href="#2-3-6-1-POLY2" class="headerlink" title="2.3.6.1 POLY2"></a>2.3.6.1 POLY2</h4><p>为了使逻辑回归能够拥有<strong>特征交叉</strong>的能力，一开始算法工程师采用人工特征交叉再筛选的方法，但是这种方式效率底下。为了能够从模型层面解决这个问题，POLY2被提了出来，公式如下：<br><img src="/img/post/rec_traditional/poly2.png" srcset="/img/loading.gif" lazyload></p>
<p>这个公式描述的是线性组合部分，后面会接Sigmoid函数，以使预测值在(0, 1)区间内。公式的前两项本质上就是逻辑回归的线性组合部分，第三项为特征交叉部分。与逻辑回顾相同的是，在该公式所描述的推荐场景中，所有离散和连续特征拼接在一起形成了总的特征向量$x$，其维度为$n$。而公式中的$x_i$则表示特征向量$x$中第$i$维的值。因此初学者在初次接触POLY2，包括后面的FM、FFM等，会对公式中特征交叉的含义产生疑惑。在公式所描述的具体实现中，$x$中的每一维都是一个特征，特征交叉即是将$x$中的所有维度进行两两交叉，而非对宏观意义上的诸如性别与职业之类的特征进行交叉。</p>
<p>可以看到该模型对所有特征进行两两交叉，并对所有特征组合赋予权重$w_{ij}$，一定程度上解决了特征交叉的问题。并且POLY2虽然通过特征交叉引入了非线性，但是从模型层面来说仍然是线性模型，与逻辑回归并无二致，便于工程上的兼容。</p>
<p>但是POLY2也存在两个弊端：</p>
<ul>
<li>评分数据原本就是非常稀疏的，而POLY2中每个特征组合都有一个相应的权重系数，稀疏数据所对应的权重无法得到有效训练，使得数据更加稀疏</li>
<li>权重参数由$n$变为$n^2$，训练复杂度大大提升</li>
</ul>
<h4 id="2-3-6-2-LS-PLM"><a href="#2-3-6-2-LS-PLM" class="headerlink" title="2.3.6.2 LS-PLM"></a>2.3.6.2 LS-PLM</h4><p>LS-PLM（Large Scale Piece-wise Linear Model）又称MLR（Mixed Logistic Regression），由阿里巴巴于2017年公开，不过早在2012年就已经是阿里内部的主流推荐模型了。它在逻辑回归的基础之上，采用分而治之的思想，将样本进行分片，然后在样本分片中使用逻辑回归进行CTR预估。</p>
<p>LS-PLM的灵感来源于对广告推荐领域样本特点的观察。比如说，如果模型要预估女性受众点击女装广告的CTR，那么我们显然不希望把男性用户点击数码产品的样本也考虑进来，这些无关样本不仅与所考虑的场景毫无关系，并且会在模型训练过程中扰乱参数的训练。因此为了针对不同用户群体和不同场景，LS-PLM先对样本进行聚类，再对每类样本使用逻辑回归进行CTR预估。</p>
<p>LS-PLM的数学形式如下：<br><img src="/img/post/rec_traditional/mlr2.png" srcset="/img/loading.gif" lazyload><br>其中$m$为分片数，即应该分为几种类别，能够较好地平衡模型的拟合和泛化能力，$m$越大，模型拟合能力越强，所需要的模型参数也越多。模型前者为一个Softmax公式，用来进行样本的多分类，后者为逻辑回归公式，用以进行CTR预估。该公式可以被抽象成如下形式：<br><img src="/img/post/rec_traditional/mlr.png" srcset="/img/loading.gif" lazyload><br>从该形式中，我们已经能够看到注意力机制的雏形了。</p>
<p>LS-PLM通过分片的方式，自动挖掘数据中的非线性模式，省去大量的人工处理和特征工程的过程，可以端到端地进行训练。此外，在实际训练中，LS-PLM还引入了L1与L2,1范数（L2,1范数是一种特殊范数），这两种范数共同使得训练出来的模型具有较高的稀疏度，因此模型的部署更加轻量，在线推断速度也会更快。</p>
<h2 id="2-4-因子分解机"><a href="#2-4-因子分解机" class="headerlink" title="2.4 因子分解机"></a>2.4 因子分解机</h2><p>为了解决POLY2的弊端，同时引进特征交叉的强大功能，Rendle于2010年提出<strong>因子分解机（Factorization Machine，简称FM）</strong>。</p>
<h3 id="2-4-1-特征交叉"><a href="#2-4-1-特征交叉" class="headerlink" title="2.4.1 特征交叉"></a>2.4.1 特征交叉</h3><p>有时候，仅利用单一特征而非交叉特征进行判断时，不仅有可能造成信息损失的问题，还有可能导致错误的结论，这便是<strong>辛普森悖论</strong>。关于辛普森悖论的例子，可以谷歌或百度进一步了解。因此在推荐中，特征数目众多，对其进行特征交叉是非常有必要的。比如，对用户来说，我们有年龄、性别和职业三种特征，那么对年龄和性别进行交叉，我们便会得到一个新的交叉特征“年龄_性别”，其取值可以是比如“男_18”，“女_22”等等。</p>
<p>从one-hot的角度来考虑特征交叉可能会更容易理解。假如我们有职业、性别两个特征，各自取值区间分别在[“医生”，“警察”，“教师”]，[“男”，“女”]，则各自所对应的one-hot向量长度分别为3和2。将职业与性别进行交叉后所得到的“职业_性别”特征，其所对应的取值就有六种，one-hot向量维度也是6，将职业和性别的所有可能取值进行笛卡尔积，便可得到“职业_性别”特征的所有可能取值。</p>
<h3 id="2-4-2-模型结构"><a href="#2-4-2-模型结构" class="headerlink" title="2.4.2 模型结构"></a>2.4.2 模型结构</h3><p>FM结合了矩阵分解和POLY2的思想，模型中引入特征交叉，但是不为每个交叉特征都学习一个新的参数。取而代之的是，FM为每个特征引入一个隐向量，然后使用两个特征的隐向量的内积作为相应交叉特征的权重，公式如下：<br><img src="/img/post/rec_traditional/fm.png" srcset="/img/loading.gif" lazyload></p>
<p>从公式中可以看出，与POLY2和逻辑回归相似的是，FM也是将原有的各个特征，比如职业、性别等，组合成一个总的特征向量$x$，然后将特征向量$x$中的每一维作为一个特征，对其进行特征交叉。因此FM是对每一维引入了一个隐向量，而非对诸如职业、性别等这样的特征引入了一个隐向量。此外，公式后面也会接一个Sigmoid用以将输出映射到(0,1)区间内。</p>
<h3 id="2-4-3-优缺点"><a href="#2-4-3-优缺点" class="headerlink" title="2.4.3 优缺点"></a>2.4.3 优缺点</h3><p>FM将矩阵分解的思想进行了扩展，从单纯地分解出用户和物品的隐向量，扩展到分解出每个特征的隐向量。此外隐向量的引入，极大地降低了模型复杂度，从POLY2的$n^2$降低到了$nk$（$k$为隐向量维度且$n$&gt;&gt;$k$）。并且由于FM的计算过程也进行了优化，其计算复杂度也被降低至$nk$，具体推导可参见这篇<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/25db169f11fa">博客</a>。</p>
<p>此外隐向量的引入能够让FM更好地处理数据稀疏的问题。因为交叉特征的权重是由两个单一特征的隐向量内积得来，因此只要其中某个特征的权重通过别的样本得到了训练，那么该交叉特征的权重就相当于进行了优化。而在POLY2模型中，一个样本必须同时满足两个单一特征取值不为0，其相应的交叉特征的权重才会被更新，但是一般情况下这种样本很少，导致交叉特征的权重参数很少被训练。此外，对于训练样本中未出现过的交叉特征，只要参与交叉的两个单一特征对应的隐向量被训练过，就可以通过内积计算该交叉特征的权重，因此FM的泛化能力也大大提高。</p>
<p>在工程方面，FM同样使用梯度下降法进行学习，并且模型容易实现，因此线上部署和推断也较为简单容易。在2012-2014年前后，成为业界主流推荐模型之一。</p>
<p>然而，FM一类模型并不能够引申到更高维度的特征交叉层面，因为三阶及以上的特征交叉会带来组合爆炸问题，导致模型复杂度和训练复杂度过高，这在实际应用中无法承受。</p>
<h3 id="2-4-4-FFM"><a href="#2-4-4-FFM" class="headerlink" title="2.4.4 FFM"></a>2.4.4 FFM</h3><p>FFM于2014年提出，并于2015年在多项CTR预估大赛中夺得冠军。FFM在FM基础上，为了进一步增强模型的学习能力，引入了<strong>特征域感知（field-aware）</strong>的概念。在理解了POLY2和FM的实际实现后，FFM的原理其实非常好懂。POLY2和FM的表述提到了两种不同的特征，一种是我们常说的比较宏观、易于理解的特征，比如性别、职业、年龄等，一种是在具体实现时所涉及的、相对更微观的特征，即将特征向量$x$的每一维作为一个特征。而FFM则是将前一种称为特征域（field），然后在引入隐向量的时候，人为考虑了不同特征域的不同影响。具体计算公式如下：<br><img src="/img/post/rec_traditional/ffm.png" srcset="/img/loading.gif" lazyload><br>可以看到，隐向量变成了$\mathbf{V}_{i,f_j}$，而非FM中的$\mathbf{V}_i$，这意味着每个特征对应着一组隐向量。当特征$x_i$与$x_j$进行交叉时，$x_i$会挑出与$x_j$的域$f_j$对应的隐向量$\mathbf{V}_{i,f_j}$进行交叉，$x_j$这边也是同理。</p>
<p>由于FFM的计算复杂度不可被优化，因此为$kn^2$。模型复杂度为$nkf$，$n$为特征个数，$k$为隐向量维度，$f$为域的数量。</p>
<p>与FM相比，FFM引入特征域的概念，使得模型的表达能力更强，但是计算复杂度也有所上升，在实际应用中，需要对二者进行权衡。</p>
<h2 id="2-5-GBDT-LR"><a href="#2-5-GBDT-LR" class="headerlink" title="2.5 GBDT+LR"></a>2.5 GBDT+LR</h2><p>FM系列模型只能处理二阶特征交叉，如果继续提高特征交叉的维度，会产生组合爆炸与计算复杂度过高的问题。因此在2014年，Facebook提出了基于GBDT+LR的组合模型来有效地进行<strong>高维特征组合和筛选</strong>。在该组合中，特征首先会通过GBDT自动进行筛选和组合，生成新的离散特征向量，然后该特征向量会输入到LR中做CTR预估。在这个过程中，GBDT用来自动进行特征工程，LR用来进行CTR预估，这两部分是独立训练的，因此无需考虑如何将LR中的梯度传播到GBDT中这种比较棘手的问题。</p>
<h3 id="2-5-1-GBDT原理"><a href="#2-5-1-GBDT原理" class="headerlink" title="2.5.1 GBDT原理"></a>2.5.1 GBDT原理</h3><p>GBDT（Gradient Boosting Decision Tree，梯度提升决策树）是集成学习boosting类别中的经典模型，由多颗决策树组成。每一颗树以前面树林的结果与真实标签的残差作为拟合目标，其生成过程是一颗标准的回归树的生成过程，因此每个节点的分裂都是一个自然的特征选择的过程。而多层节点则对特征进行了有效的自动组合，因此高效地解决了特征选择和特征组合的问题。</p>
<h3 id="2-5-2-如何进行特征组合"><a href="#2-5-2-如何进行特征组合" class="headerlink" title="2.5.2 如何进行特征组合"></a>2.5.2 如何进行特征组合</h3><p>首先使用训练集对GBDT进行训练。训练过程中，GBDT预测输出的并不是一个二分类概率值。对于每一个训练样本，其在每一颗树中最终落到的叶子节点会被设置为1，其余叶子节点会被设置为0，所有树的叶子节点组合在一起便形成了一个用以输入到LR模型的离散型特征向量。</p>
<p>在这个过程中，GBDT中决策树的深度决定了特征组合的阶数。假如深度为4，则通过3次节点分裂，最终的叶子节点实际上是进行三阶特征组合后的结果。</p>
<h3 id="2-5-3-优缺点"><a href="#2-5-3-优缺点" class="headerlink" title="2.5.3 优缺点"></a>2.5.3 优缺点</h3><p>GBDT+LR让模型自动完成特征工程，使得算法工程师不必在特征工程上花费过多精力，实现了真正的<strong>端到端（End2End）</strong>的训练。而在这之前，特征工程的主要方法有两个，一个是人工特征组合和筛选，一个是改进目标结构、损失函数或增加特征交叉项。前者对工程师的经验和精力要求较高，后者对模型设计能力要求较高。</p>
<p>但是GBDT无法进行完全并行的训练，训练时间较长，并且GBDT自身没有诸如正则化项之类的防止过拟合的有效手段，因此容易出现过拟合。并且虽然GBDT理论上能够进行高阶的特征组合，但是GBDT的特征组合过程丢失了大量的数值信息，因此GBDT并不一定就比FM系列要好。</p>
<h1 id="3-结语"><a href="#3-结语" class="headerlink" title="3 结语"></a>3 结语</h1><p>在深度学习大行其道的今天，各种模型五花八门，从今天回望过去，以上所介绍的模型可能过于单一和乏味。尽管如此，这些模型仍然值得回味和学习，其中所蕴含的思想仍然指导着如今大多数推荐模型的演化和发展，这大概就是经典永流传吧。在后续的文章中，我将进一步整理深度学习时代的经典模型，希望能够坚持把这个系列整理完。</p>
<p>参考文献：</p>
<section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1423975">Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions</a>
<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%8D%94%E5%90%8C%E9%81%8E%E6%BF%BE">协同过滤维基百科</a>
<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>
              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/recommendation/" class="category-chain-item">recommendation</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/machine-learning/">#machine learning</a>
      
        <a href="/tags/recommendation/">#recommendation</a>
      
    </div>
  
</div>


              

              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/posts/26532/" title="推荐之Embedding技术">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">推荐之Embedding技术</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/posts/2110556878/" title="推荐之框架理解">
                        <span class="hidden-mobile">推荐之框架理解</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  




  


  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>






  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        MathJax = {
          tex    : {
            inlineMath: { '[+]': [['$', '$']] }
          },
          loader : {
            load: ['ui/lazy']
          },
          options: {
            renderActions: {
              findScript    : [10, doc => {
                document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                  const display = !!node.type.match(/; *mode=display/);
                  const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                  const text = document.createTextNode('');
                  node.parentNode.replaceChild(text, node);
                  math.start = { node: text, delim: '', n: 0 };
                  math.end = { node: text, delim: '', n: 0 };
                  doc.math.push(math);
                });
              }, '', false],
              insertedScript: [200, () => {
                document.querySelectorAll('mjx-container').forEach(node => {
                  let target = node.parentNode;
                  if (target.nodeName.toLowerCase() === 'li') {
                    target.parentNode.classList.add('has-jax');
                  }
                });
              }, '', false]
            }
          }
        };
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.0/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
