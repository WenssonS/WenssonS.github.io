<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>推荐之精排模型</title>
    <link href="/posts/undefined/"/>
    <url>/posts/undefined/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>推荐之Embedding技术</title>
    <link href="/posts/26532/"/>
    <url>/posts/26532/</url>
    
    <content type="html"><![CDATA[<h1 id="0-写在前面"><a href="#0-写在前面" class="headerlink" title="0 写在前面"></a>0 写在前面</h1><p>上一篇post着重探讨了传统推荐模型的分类和演进，其中在讨论矩阵分解时讨论到了Embedding技术，当然在矩阵分解刚提出的年代，推荐领域并没有引入Embedding的概念和技术，当时还只是将这种东西称为隐向量。然而随着2012年之后深度学习的流行以及2013年Word2vec技术的提出，Embedding技术迅速发展了起来，在被引入到推荐领域之后，逐渐成为推荐领域的基石之一。因此，很有必要将Embedding单独拎出来，花上一个post的篇幅来着重进行探讨。</p><h1 id="1-Embedding"><a href="#1-Embedding" class="headerlink" title="1 Embedding"></a>1 Embedding</h1><h2 id="1-1-Embedding起源"><a href="#1-1-Embedding起源" class="headerlink" title="1.1 Embedding起源"></a>1.1 Embedding起源</h2><p>Embedding起源于自然语言处理（Natural Language Processing，简称NLP）领域。其实从这句话开始，包括在后续许多的推荐模型中，我们会越来越感受到，<strong>推荐领域的很多灵感都来自于NLP领域</strong>。与图像这种自然信号不同的是，文字属于一种人造信号，信息密度比较高，从人工智能领域的角度讲，一个是“感知智能”，一个是“认知智能”。图像在计算机中以pixel为单位进行表示和存储，本身就是数字信息，可直接使用机器学习模型处理，并且单个pixel不具备什么语义信息，只有将局部或全局的pixel聚合起来，才会具备一定的语义信息。而在文本中，每个词或字自身便携带语义信息，且不容易通过数学的方式对其进行语义上的表示，因此NLP领域很早就开始了如何用向量的形式表示一个词。当然早期基本只有英语系的学者从事这方面的研究，很少会涉及到诸如中文里的字这种概念，因此这种技术一般被称为词嵌入或词向量（Word Embedding）。</p><p>在如何用向量表达词的问题上，最直观的想法是使用<strong>独热编码（One-Hot Encoding）</strong>，即有多少词，就使用一个多长的向量来对词进行表示，对于每一个词，只有其对应的index上的元素为1，其余元素为0。因此one-hot方法所得到的向量非常稀疏，并且不具备语义信息。为了克服这种缺点，Hinton等人于1986年提出了一个开创性的概念——<strong>分布式表示（Distributed Representation）</strong>的概念，即将每一个词映射到一个固定长度的短向量上，来解决one-hot方法的不足。而最早提出<strong>使用神经网络来训练语言模型</strong>的思想，则是由百度深度学习研究院的徐伟于2000年提出。随后在2003年，Bengio等人在提出<strong>神经网络语言模型（Neural Network Language Model，简称NNLM）</strong>来学得单词的分布式表示。而随着2013年<strong>Word2vec</strong>的提出，Word Embedding才真正大火了起来。</p><p>差不多在同一时间，其他领域也逐渐开始进行Embedding方法的研究。在知识图谱（Knowledge Graph，简称KG）领域，为了更好地对三元组进行建模，学得更好的实体和关系的Embedding，TransE被提了出来。2014年，图表示学习领域也出现了一个非常有影响力的工作，即DeepWalk，其本质是RandomWalk+Word2vec。而在推荐领域，Embedding的思想早在矩阵分解的年代就已经被挖掘了出来。并且在深度学习时代之后，随着Deep Crossing、Wide&amp;Deep、Item2vec等一众优秀模型的提出，Embedding也迅速与神经网络模型和推荐领域深度绑定在了一起。</p><p>自Word2vec问世以来，各种各样的Embedding模型层出不穷，深度学习圈子里甚至有人表示“万物皆可Embedding”。深度学习之所以能够取得今天这样的辉煌，离不开其强大的表示学习能力，而Embedding所做的事情本质上就是表示学习，其强大的特征提取和抽象的能力，使得它可以用来表示几乎任何对象，并能与后面的神经网络模型无缝衔接，进一步提高表示学习的能力。</p><h2 id="1-2-何为Embedding"><a href="#1-2-何为Embedding" class="headerlink" title="1.2 何为Embedding"></a>1.2 何为Embedding</h2><p>Embedding，直译为“嵌入”，不过翻译成“嵌入表示”或者“向量表示”其实更为贴近也更好理解，即<strong>用低维稠密的向量表示一个对象（object），如单词、句子、图像、用户或物品等。</strong>不考虑预训练等技术，随机初始化的Embedding一般并不携带任何语义信息，然而在给定适当的训练数据、模型和优化目标并进行一定的训练后，Embedding便能够抽象地描述一个对象的特点。作为深度学习中的重要一环，Embedding完美展现了深度学习的从“精确查找”转变到“模糊匹配”的思想特点，同时也是深度学习即表示学习的最好体现。</p><h2 id="1-3-推荐为什么需要Embedding"><a href="#1-3-推荐为什么需要Embedding" class="headerlink" title="1.3 推荐为什么需要Embedding"></a>1.3 推荐为什么需要Embedding</h2><p>一般推荐场景中的数据非常稀疏，且诸如类别、ID等离散特征会先进行one-hot编码处理，这种情况下加入Embedding层之后可带来如下好处：</p><ul><li>大量特征one-hot后，所得到的特征高维且稀疏，加之训练数据往往也是非常稀疏的，因此在训练过程中，特征规模过大导致模型的训练效果非常差。而将one-hot向量转化成Embedding后，可以极大缓解训练不充分的问题。</li><li>Embedding的表达能力比传统方法所得到的特征向量更强，能捕捉到一些高阶的（参考FM的初衷）、抽象的内在特征，能帮助提高模型的性能。</li><li>Embedding可被用来直接表示用户和物品，因此可以通过使用Embedding进行相似度计算等方式，进行推荐的召回。</li><li>由于Embedding自身便可携带语义信息，因此其使用非常灵活，可与推荐模型无缝衔接。Embedding可以与模型一起训练（大部分推荐模型），也可以预训练好后用来初始化其他推荐模型（如FNN）或被多个模型所共享（如ESMM、MMOE等）。Embedding的这种特性可以与诸如迁移学习等技术融合，共同缓解冷启动问题。</li></ul><h1 id="2-NLP中的Embedding"><a href="#2-NLP中的Embedding" class="headerlink" title="2 NLP中的Embedding"></a>2 NLP中的Embedding</h1><p>NLP领域的Embedding技术在整个深度学习领域一直处于一个比较领先的位置，其中的很多其他领域的模型都是借鉴了NLP中相应的技术方法。</p><h2 id="2-1-经典词向量方法"><a href="#2-1-经典词向量方法" class="headerlink" title="2.1 经典词向量方法"></a>2.1 经典词向量方法</h2><h3 id="2-1-1-Word2vec"><a href="#2-1-1-Word2vec" class="headerlink" title="2.1.1 Word2vec"></a>2.1.1 Word2vec</h3><p>Word2vec的核心思想是<strong>具有相似上下文的单词具有相似的Embedding</strong>，本质上属于一种无监督训练模型，其基本框架如下<br><img src="/img/post/rec_embedding/word2vec.png" alt="Word2vec基本框架"><br>从图中可以看到，Word2vec模型本质是一个三层的神经网络，输入层为单词的one-hot向量，通过$W_{V \times N}$矩阵转换成中间的隐层向量，再通过另一个矩阵映射到输出层，并通过softmax输出各单词的概率。根据这个大体框架和Word2vec的核心思想，Word2vec的作者提出了CBOW与skip-gram两个具体的实现。</p><h4 id="2-1-1-1-CBOW与skip-gram"><a href="#2-1-1-1-CBOW与skip-gram" class="headerlink" title="2.1.1.1 CBOW与skip-gram"></a>2.1.1.1 CBOW与skip-gram</h4><p>CBOW是通过上下文预测当前的单词，模型框架如下：<br><img src="/img/post/rec_embedding/cbow.jpg" alt="CBOW模型图"></p><p>skip-gram是通过当前单词预测上下文，模型框架如下：<br><img src="/img/post/rec_embedding/skip-gram.png" alt="skip-gram模型图"></p><p>在实际训练的时候，两个模型中都引入了窗口的概念，通过某个中心词与其前n个、其后n个单词所形成的一个窗口来表示一个上下文，然后在语料中大量采样这样的窗口来作为训练数据。有了大量的上下文窗口之后，根据Word2vec的核心思想，CBOW的优化目标是给定上下文单词最大化中心单词的概率，skip-gram则是给定中心单词最大化上下文单词的概率。以skip-gram为例，其损失函数如下：<br><img src="/img/post/rec_embedding/skip-gram_loss.png"><br>其中概率计算如下：<br><img src="/img/post/rec_embedding/skip-gram_loss2.png"><br>在训练结束后，$W_{V \times N}$这个矩阵其实就是想要获得的词向量矩阵。</p><h4 id="2-1-1-2-优化策略"><a href="#2-1-1-2-优化策略" class="headerlink" title="2.1.1.2 优化策略"></a>2.1.1.2 优化策略</h4><p>在输出层计算某单词的概率的时候，使用的是softmax函数。而在实际场景中，往往单词数量特别大，使用softmax的计算开销则会变得不可接受，同时模型的训练难度也大大增加。针对这个问题，主要有负采样（Negativve sampling）和层级Softmax（Hierarchical Softmax）两种解决方法。</p><p>负采样是NCE（Noise-Constrastive Estimation）的一种简化版，以skip-gram为例，其思想是采样与当前中心词不处于同一上下文的单词，将它们组成负例样本，对于每一对正负例，最小化正例中单词共现的概率，最小化负例中单词共现的概率，以此将原有的多分类问题转化成多个二分类问题，这能够降低计算复杂度，同时提高模型训练效果。skip-gram采用负采样后的损失函数如下：<br><img src="/img/post/rec_embedding/negative_sample_loss.png"><br>其中$D$和$D’$分别表示正例集和负例集。</p><p>层级softmax则是利用霍夫曼树（Huffman Tree）替代从隐藏层到softmax输出这一过程。该霍夫曼树的每个叶子节点表示一个词，由于霍夫曼树的结构特点，频率高的词会尽可能地靠近根节点。树中每个节点都有相应的可训练参数，用以参与sigmoid计算，得出在当前节点往左右子树游走的概率，每个单词的概率即由从根节点到该叶子结点的路径上各sigmoid值的乘积。在这个过程中，原有softmax的操作被路径上的多个sigmoid所替代，因此该方法被称作层级softmax。这种方式避免了softmax中需要对每个单词进行概率计算的开销，原有的计算量可以从$O(V)$降低到$O(log_2V)$。</p><h3 id="2-1-2-Glove"><a href="#2-1-2-Glove" class="headerlink" title="2.1.2 Glove"></a>2.1.2 Glove</h3><h3 id="2-1-3-Fasttext"><a href="#2-1-3-Fasttext" class="headerlink" title="2.1.3 Fasttext"></a>2.1.3 Fasttext</h3><h3 id="2-1-4-ELMo"><a href="#2-1-4-ELMo" class="headerlink" title="2.1.4 ELMo"></a>2.1.4 ELMo</h3><h2 id="2-2-预训练语言模型"><a href="#2-2-预训练语言模型" class="headerlink" title="2.2 预训练语言模型"></a>2.2 预训练语言模型</h2><p>从2018年开始，NLP领域涌现出了几个预训练语言模型，其中非常有代表性的当属BERT和GPT，随后涌现出了大量基于BERT和GPT的变体模型。<br>BERT的主要改进点 1实现真正双向多层语言模型 2显式地对句子关系建模</p><h1 id="3-Graph上的Embedding"><a href="#3-Graph上的Embedding" class="headerlink" title="3 Graph上的Embedding"></a>3 Graph上的Embedding</h1><h2 id="3-1-经典Graph-Embedding方法"><a href="#3-1-经典Graph-Embedding方法" class="headerlink" title="3.1 经典Graph Embedding方法"></a>3.1 经典Graph Embedding方法</h2><h3 id="3-1-1-DeepWalk"><a href="#3-1-1-DeepWalk" class="headerlink" title="3.1.1 DeepWalk"></a>3.1.1 DeepWalk</h3><h3 id="3-1-2-LINE"><a href="#3-1-2-LINE" class="headerlink" title="3.1.2 LINE"></a>3.1.2 LINE</h3><h3 id="3-1-3-Node2vec"><a href="#3-1-3-Node2vec" class="headerlink" title="3.1.3 Node2vec"></a>3.1.3 Node2vec</h3><h3 id="3-1-4-SDNE"><a href="#3-1-4-SDNE" class="headerlink" title="3.1.4 SDNE"></a>3.1.4 SDNE</h3><h2 id="3-2-图神经网络方法"><a href="#3-2-图神经网络方法" class="headerlink" title="3.2 图神经网络方法"></a>3.2 图神经网络方法</h2><h1 id="4-推荐中的Embedding"><a href="#4-推荐中的Embedding" class="headerlink" title="4 推荐中的Embedding"></a>4 推荐中的Embedding</h1><h1 id="5-KG中的Embedding"><a href="#5-KG中的Embedding" class="headerlink" title="5 KG中的Embedding"></a>5 KG中的Embedding</h1><p>KG于2012年被Google提出，是一种比较特殊的图，其上的节点被称为实体（Entity），边携带语义信息被称为关系（Relation）。虽然KG本质上属于异构图（Heterogeneous Graph或Heterogeneous Information Network），但是一般情况下，KG中的实体类型和关系类型的种类非常丰富，而异构图领域所研究的图比较简单，因此两者在实际的研究和应用中仍旧存在着较大差别。</p><p>在2013年TransE模型被提出之后，越来越多的人开始研究如何更好地对KG中的实体和关系进行Embedding。由于现在学术界和工业界也有一些将KG引入推荐场景增强推荐性能的尝试，因此本小节对KG Embedding中比较经典的方法做一个小小的分类介绍。</p><h2 id="5-1-基于距离的模型"><a href="#5-1-基于距离的模型" class="headerlink" title="5.1 基于距离的模型"></a>5.1 基于距离的模型</h2><h2 id="5-2-双线性模型"><a href="#5-2-双线性模型" class="headerlink" title="5.2 双线性模型"></a>5.2 双线性模型</h2><h2 id="5-3-神经网络模型"><a href="#5-3-神经网络模型" class="headerlink" title="5.3 神经网络模型"></a>5.3 神经网络模型</h2><h2 id="5-4-基于旋转的模型"><a href="#5-4-基于旋转的模型" class="headerlink" title="5.4 基于旋转的模型"></a>5.4 基于旋转的模型</h2>]]></content>
    
    
    <categories>
      
      <category>recommendation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>deep learning</tag>
      
      <tag>recommendation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>推荐之传统模型</title>
    <link href="/posts/1246173879/"/>
    <url>/posts/1246173879/</url>
    
    <content type="html"><![CDATA[<h1 id="0-写在前面"><a href="#0-写在前面" class="headerlink" title="0 写在前面"></a>0 写在前面</h1><p><strong>推荐系统</strong>是一种信息过滤系统，通过预测用户对物品的评分或偏好，来推荐用户未交互过的物品。推荐的对象包括但不限于：视频、音乐、新闻、书籍、电商平台中的各类商品等。</p><p>在过去的二三十年里，伴随着互联网的快速兴起，海量的信息和数据呈现爆发式的增长，这被人们称为<strong>信息超载（Information Overload）</strong>。为应对信息超载问题，推荐系统得到了十分迅猛的发展，衍生出了非常多的模型和方法。由于推荐系统本身可以从多个角度进行衡量和评价，并且多种推荐算法之间往往存在演进与结合的关系，其分界线并不十分清晰，因此对其进行系统而统一的梳理分类是一件比较头疼的事情。</p><p><strong>该post致力于整理深度学习大行其道之前的这段时间里，推荐系统领域的主流方法或模型，并将与接下来几个post共同整理推荐系统领域的一些常用算法模型和问题解法。由于推荐系统领域的复杂和多样，笔者深知区区几篇post是不可能整理完所有内容，因此将按笔者所了解及感兴趣的思路来进行整理。</strong></p><h1 id="1-传统推荐算法的分类"><a href="#1-传统推荐算法的分类" class="headerlink" title="1 传统推荐算法的分类"></a>1 传统推荐算法的分类</h1><p>传统的推荐方法一般可分为<strong>基于内容（Content-based）的推荐</strong>、<strong>基于协同过滤（Collaborative Filtering）的推荐</strong>及<strong>混合式（Hybrid）的推荐</strong>。<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions">[1]</span></a></sup> 至于基于规则（Rule-based）或者基于人口统计信息（Demographic-based）等其他较为小众的类别，本文不作介绍。下图展示了大概的分类框架：</p><p><img src="/img/post/rec_traditional/traditional_rec_classification.jpg" alt="传统推荐算法分类"></p><h2 id="1-1-基于内容的推荐"><a href="#1-1-基于内容的推荐" class="headerlink" title="1.1 基于内容的推荐"></a>1.1 基于内容的推荐</h2><p>顾名思义，该方法专注于通过分析物品的内容或属性，找到一个用户过去所交互过的物品的共性，来找到该用户可能感兴趣的物品。这类方法起源于检索领域，因此主要考虑文本信息，比如文档、URL、TF-IDF等。用户的偏好即通过其所交互过的物品的内容信息聚合而来，通过用户偏好与物品的内容信息来对未交互过的物品进行分数预测。当然，在具体计算的过程中，也有些方法使用如朴素贝叶斯、决策树、神经网络等整合内容信息，来预测用户对一个物品的分数。</p><p>该方法忽略用户行为，只考虑物品与物品的相关性，因此不太受物品冷启动问题的影响。并且整个过程简单，可离线进行，因此推荐响应速度快，简洁有效。此外，由于每个用户和物品都由内容属性来表示，因此具有比较好的可解释性。</p><p>该方法只能对文本进行较为有效的分析，除了文本，诸如图像、音频、视频等内容信息都不太好提取其特征。并且即使在处理文本时，特征的提取也需要仔细地设计，比如使用关键词信息表示物品，容易出现两个物品的内容信息一致的情况，进而无法对其进行有效区分。特征抽取方面的困难，限制了该方法的上限。此外，该方法的推荐多样性不够好，这会使推荐的物品只与用户所交互过的物品具有强相关性，而其他的物品则由于内容差别过大而不会被推荐。当然，除了上述问题，该方法也深受用户冷启动问题的困扰。</p><h2 id="1-2-基于协同过滤的推荐"><a href="#1-2-基于协同过滤的推荐" class="headerlink" title="1.2 基于协同过滤的推荐"></a>1.2 基于协同过滤的推荐</h2><p>协同过滤本质上是一大类推荐算法，目前比较主流的看法是将基于协同过滤的推荐进一步细分为<strong>基于存量（Memory-based）的协同过滤</strong>和<strong>基于模型（Model-based）的协同过滤</strong>，而基于存量（Memory-based）的协同过滤则进一步细分为大家所熟知的<strong>基于用户（User-based）的协同过滤</strong>和基于<strong>物品（Item-based）的协同过滤</strong>。<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="协同过滤维基百科">[2]</span></a></sup></p><p>一般所讲的协同过滤专指基于存量的协同过滤，即UserCF和ItemCF，这可以算是狭义上的协同过滤。而基于模型的协同过滤算法，主要是通过用户-物品评分矩阵训练一个机器学习模型，然后输入用户和物品ID得到评分预测。因此在讨论协同过滤的时候，很容易陷入一个误区，即以为只有UserCF与ItemCF两种协同过滤方法。然而由于协同过滤的定义十分宽泛，因此事实上几乎所有的推荐算法，比如早期的矩阵分解、逻辑回归，以及现有的各类神经网络模型，都可以被看作是基于模型的协同过滤的变种或改进算法，这种体会会随着对推荐算法的了解而逐渐加深。本文只讨论传统的推荐模型，为了让讨论更有章可循，<strong>在该post的以下内容中，我们只讨论狭义上的协同过滤，所讲的协同过滤仅指代基于存量的协同过滤，即UserCF与ItemCF。</strong></p><p>协同过滤算法简单有效，工程上容易实现，不需要过多领域知识，与物品本身的内容属性无关，无需考虑对内容的特征抽取过程。但是该方法受数据稀疏、冷启动问题的困扰，模型泛化能力不足，并且不能够引入其他对推荐有用的特征。</p><h2 id="1-3-混合式的推荐"><a href="#1-3-混合式的推荐" class="headerlink" title="1.3 混合式的推荐"></a>1.3 混合式的推荐</h2><p>与其说混合式推荐是一种分类，不如说现实中的推荐系统大多都是这样设计的。其思想即为组合多种推荐方法，将它们的优点结合起来进行互补，以达到更好的推荐效果，具体方法这里不再赘述。</p><h1 id="2-传统推荐算法的演进"><a href="#2-传统推荐算法的演进" class="headerlink" title="2 传统推荐算法的演进"></a>2 传统推荐算法的演进</h1><p>在讨论完传统推荐算法的分类后，接下来会按照算法模型的演进过程，讨论一些经典推荐算法的细节问题。传统推荐算法以协同过滤为出发点，衍生出了两大类经典方法，一类着力于提升协同过滤的学习能力和泛化能力，发展出了<strong>矩阵分解（Matrix Factorization，简称MF）</strong>方法；一类研究如何更好地引入更多的特征，发展出了<strong>逻辑回归（Logistic Regression，简称LR）</strong>方法。后续方法，包括现今各类的神经网络模型，也是基于这两条路进行模型层面的改进。下图借鉴了王喆的《深度学习推荐系统》的框架图，展示了该演化过程：</p><p><img src="/img/post/rec_traditional/traditional_rec_evolution.jpg" alt="传统推荐算法演进"></p><h2 id="2-1-协同过滤"><a href="#2-1-协同过滤" class="headerlink" title="2.1 协同过滤"></a>2.1 协同过滤</h2><p>如前文所述，这里讨论UserCF与ItemCF，是由亚马逊于2003年提出的两个最经典的协同过滤算法。</p><h3 id="2-1-1-基于用户和物品的协同过滤"><a href="#2-1-1-基于用户和物品的协同过滤" class="headerlink" title="2.1.1 基于用户和物品的协同过滤"></a>2.1.1 基于用户和物品的协同过滤</h3><p>两种协同过滤算法本质上是一样的，并且都需要用到用户-物品评分矩阵，区别在于两者分别从用户和物品的角度进行计算。评分矩阵中，每一行为一个用户向量，每一列为一个物品向量。</p><p>UserCF过程如下：</p><ul><li>根据用户交互记录，计算用户相似度</li><li>找到与目标用户最相似的K个用户</li><li>在这K个用户所喜欢或交互过的物品中，找到目标用户未交互过的进行推荐</li></ul><p> ItemCF过程如下：</p><ul><li>根据物品交互记录，计算物品相似度</li><li>找到与目标物品最相似的K个物品</li><li>在这K个物品中，找到与这些物品交互过且未与目标物品交互过的用户，将目标物品推荐给这些用户</li></ul><h3 id="2-1-2-相似度计算公式"><a href="#2-1-2-相似度计算公式" class="headerlink" title="2.1.2 相似度计算公式"></a>2.1.2 相似度计算公式</h3><p>协同过滤的核心在于如何更好地计算相似度，如下给出几种常用方法。</p><h4 id="2-1-2-1-闵可夫斯基距离"><a href="#2-1-2-1-闵可夫斯基距离" class="headerlink" title="2.1.2.1 闵可夫斯基距离"></a>2.1.2.1 闵可夫斯基距离</h4><p>闵可夫斯基距离（Minkowski Distance）其实是一类距离的统称，其计算方式与向量的范数计算本质上是一样的：<br><img src="/img/post/rec_traditional/osjl.png"></p><p>当p&#x3D;1时，计算的便是曼哈顿距离（Manhattan Distance）。<br>当p&#x3D;2时，计算的便是<strong>欧氏距离（Euclidean Distance）</strong>。欧式距离为最常见的相似度计算方式，易于理解和实现，但是只适用于欧式空间。不过在协同过滤的场景中，已经能够满足需求。<br>当p趋于无穷时，计算的便是切比雪夫距离（Chebyshev Distance）。</p><p>欧式距离默认每个维度的权重都是相同的，因此当某个维度的值非常大时，非常容易由该维度主导整个计算结果。因此在计算欧氏距离之前，最好是先对数据做归一化处理。</p><h4 id="2-1-2-2-余弦相似度"><a href="#2-1-2-2-余弦相似度" class="headerlink" title="2.1.2.2 余弦相似度"></a>2.1.2.2 余弦相似度</h4><p><strong>余弦相似度（Cosine Similarity）</strong>取值区间在[-1, 1]之间，越靠近1表明两个向量相似度越高<br><img src="/img/post/rec_traditional/cxsd.png"></p><p>相比于欧氏距离，余弦相似度计算的是一种相对的距离，与向量的幅度无关，只与其方向有关。在关注绝对差异时，比如用户的活跃度时，则欧氏距离能够比较好地衡量活跃度差异。但是在关注相对差异时，比如两个用户的物品交互记录分别为[3, 5, 3]和[4, 6, 4]，他们的兴趣应该是差不多相似的，但是欧氏距离并不能很好地计算他们的相似度。在推荐场景中，用户和物品常常存在<strong>偏置问题（bias）</strong>，余弦相似度能够起到一些<strong>去偏（debias）</strong>的作用。</p><p>余弦相似度计算比较直接，容易受到维度缺失的影响。而在实际的推荐场景中，不可避免地会出现某些维度的值为null的情况。</p><h4 id="2-1-2-3-皮尔逊相似度"><a href="#2-1-2-3-皮尔逊相似度" class="headerlink" title="2.1.2.3 皮尔逊相似度"></a>2.1.2.3 皮尔逊相似度</h4><p>在余弦相似度基础上，每个向量减去这个向量均值组成的向量，便是<strong>皮尔逊相似度（Pearson Correlation Similarity）</strong>，也称皮尔逊相关系数。<br><img src="/img/post/rec_traditional/pexxsd.png"></p><p>相比于欧氏距离，皮尔逊相似度对不同变量的取值范围没有要求，变量的量纲差别在计算过程中被去掉了，等价于z-score标准化。</p><p>相比于余弦相似度，皮尔逊相似度相当于在计算余弦相似度之前，先做了中心化的操作。在中心化时，默认会将所有缺失的维度填上0值，因此能够解决余弦相似度解决不了的维度缺失问题。当两个向量均值为0时，皮尔逊相似度即余弦相似度。在推荐场景中，中心化操作也使皮尔逊相似度能够更好地起到去偏的作用。</p><h4 id="2-1-2-4-Jaccard相似度"><a href="#2-1-2-4-Jaccard相似度" class="headerlink" title="2.1.2.4 Jaccard相似度"></a>2.1.2.4 Jaccard相似度</h4><p><strong>Jaccard相似度（Jaccard Similarity）</strong>是衡量两个集合相似度的方法，也可以用于协同过滤场景。<br><img src="/img/post/rec_traditional/jxsd.png"></p><h3 id="2-1-3-两种协同过滤的区别"><a href="#2-1-3-两种协同过滤的区别" class="headerlink" title="2.1.3 两种协同过滤的区别"></a>2.1.3 两种协同过滤的区别</h3><p>虽然两者过程相似，但是在工业界的实际使用中两者区别还是蛮大的，ItemCF的使用要明显多于UserCF。</p><p>UserCF：</p><ul><li>用户数较少的场景，复杂度O(M<sup>2</sup>N)，M为用户数，N为物品数</li><li>推荐偏热门，多样性较差</li><li>时效性强，用户个性化需求不太明显，推荐粒度较粗</li><li>可解释性不好</li><li>新闻、社交领域等</li></ul><p>ItemCF：</p><ul><li>物品数较少的场景，复杂度O(N<sup>2</sup>M)，M为用户数，N为物品数</li><li>推荐偏个性化，多样性较好</li><li>用户兴趣比较固定，个性化需求强，推荐粒度较细</li><li>可解释性较好</li><li>电商、电影、书籍等</li></ul><h3 id="2-1-4-优缺点"><a href="#2-1-4-优缺点" class="headerlink" title="2.1.4 优缺点"></a>2.1.4 优缺点</h3><p>优点：</p><ul><li>简单有效，易于实现</li><li>可离线生成计算结果，容易上线</li></ul><p>缺点：</p><ul><li>受冷启动问题的制约，对冷启动用户和物品的推荐效果不好</li><li>头部效应明显，处理稀疏向量的能力比较弱，泛化能力比较差</li><li>仅考虑历史交互信息，无法引入用户和物品的属性、上下文等很多对推荐有用的信息，可扩展性不好</li></ul><h2 id="2-2-矩阵分解"><a href="#2-2-矩阵分解" class="headerlink" title="2.2 矩阵分解"></a>2.2 矩阵分解</h2><p>矩阵分解由Netflix于2006年提出。矩阵分解在协同过滤的基础上，加入了<strong>隐向量（Latent Vector）</strong>的概念来表示用户和物品，这加强了模型处理稀疏矩阵的能力，同时也增加了模型的学习能力与泛化能力。矩阵分解的提出同样意义非凡，如今几乎每一个推荐模型都会为用户和物品学习一套<strong>Embedding</strong>，即矩阵分解中的隐向量，用以表示它们的偏好特点。而用户对物品的预测分数，则一般使用两个Embedding的内积来表示。尽管在矩阵分解早期，推荐领域并没有Embedding的概念，但是从现今的角度回顾矩阵分解技术，将隐向量称为Embedding并不会引起歧义，因此为表述方便本小节统称为Embedding。</p><h3 id="2-2-1-经典方法"><a href="#2-2-1-经典方法" class="headerlink" title="2.2.1 经典方法"></a>2.2.1 经典方法</h3><h4 id="2-2-1-1-SVD"><a href="#2-2-1-1-SVD" class="headerlink" title="2.2.1.1 SVD"></a>2.2.1.1 SVD</h4><p>奇异值分解（Singular Value Decomposition，简称SVD），其公式如下：<br><img src="/img/post/rec_traditional/svd.jpg"><br>分解完成后，矩阵$\mathbf{U}$与矩阵$\mathbf{V}$则分别表示用户和物品的Embedding矩阵。如果只取较大的<em>k</em>个奇异值，则两个矩阵中的Embedding维度也会被缩小至<em>k</em>。</p><p>虽然看起来SVD的理论能够完美套用在矩阵分解中，但是在实际场景中，SVD不可避免地会遇到如下两个问题：</p><ul><li>SVD的前提是矩阵是稠密的，然而实际评分矩阵非常稀疏。因此在使用SVD之前，必须对评分矩阵进行补全，如平均值等方法。但是补全又会带来两个问题，一个是增加算法复杂度，另一个是简单粗暴的补全会引入较为明显的误差。</li><li>传统的SVD的计算复杂度达到了O(MN<sup>2</sup>)，这种计算复杂度在实际推荐场景中是不可接受的。</li></ul><h4 id="2-2-1-2-FunkSVD"><a href="#2-2-1-2-FunkSVD" class="headerlink" title="2.2.1.2 FunkSVD"></a>2.2.1.2 FunkSVD</h4><p>FunkSVD是最经典的矩阵分解模型，也是最早的<strong>隐语义模型（Latent Factor Model）</strong>，它将评分矩阵直接分解为用户和物品两个Embedding矩阵，每个用户和物品通过其对应的Embedding来表示：<br><img src="/img/post/rec_traditional/funksvd.png"><br>在具体分解矩阵的时候，FunkSVD采用<strong>梯度下降法（Gradient Descent）</strong>进行Embedding的优化，损失函数如下：<br><img src="/img/post/rec_traditional/funksvd_loss.png"><br>FunkSVD使用用户和物品的Embedding内积作为预测得分，并且损失函数中还加入了L2正则化项防止过拟合。在FunkSVD的身上，我们已经能够看到现在的推荐算法的雏形了。</p><h4 id="2-2-1-3-BiasSVD"><a href="#2-2-1-3-BiasSVD" class="headerlink" title="2.2.1.3 BiasSVD"></a>2.2.1.3 BiasSVD</h4><p>BiasSVD主要是为了处理推荐场景中的偏置问题，在FunkSVD基础上，BiasSVD考虑如下三个bias部分：</p><ul><li>训练集中所有评分记录的平均数$\mu$</li><li>用户偏置$b_u$，表示用户$u$的打分习惯，有些用户可能倾向于打分高，有些用户可能倾向于打分低</li><li>物品偏置$b_i$，表示某一物品$i$得到的打分情况，与上一条相似</li></ul><p> 考虑偏置后的预测分数计算如下<br><img src="/img/post/rec_traditional/biassvd.png"><br>考虑偏置后的损失函数如下：<br><img src="/img/post/rec_traditional/biassvd_loss.png"></p><h4 id="2-2-1-4-SVD"><a href="#2-2-1-4-SVD" class="headerlink" title="2.2.1.4 SVD++"></a>2.2.1.4 SVD++</h4><p>SVD++在BiasSVD的基础上又添加了用户的隐式反馈信息，如用户的浏览记录、收听记录等，相当于引入额外信息，帮助解决显式评分行为较少而导致的冷启动问题。</p><p>SVD++认为除了用户显式评分过的物品，有过行为的物品也都有Embedding表示。此外，用户的一些属性，比如社会统计学信息，转成01特征后，也都使其对应一个Embedding。这两者各自相加，表示一部分该用户的偏好：<br><img src="/img/post/rec_traditional/svd++.png"><br>图中，$\mathbf{N}(u)$表示与用户$u$有过行为交互的物品集合，$\mathbf{A}(u)$表示用户$u$的属性集合。</p><h4 id="2-2-1-5-TimeSVD"><a href="#2-2-1-5-TimeSVD" class="headerlink" title="2.2.1.5 TimeSVD"></a>2.2.1.5 TimeSVD</h4><p>TimeSVD在BiasSVD基础上，加入了时间变化的影响，该模型认为物品的流行度以及用户的偏好都是会随时间流逝的， 计算公式如下：<br><img src="/img/post/rec_traditional/timesvd.png"><br>其中$b_u(t)$、$b_i(t)$和$p_u(t)$分别是用户偏置、物品偏置和用户隐向量随时间变化的函数，TimeSVD不考虑物品隐向量的时间变化。在具体计算时间变化时，TimeSVD采用时间离散化或者高斯核来进行处理。</p><h3 id="2-2-2-优缺点"><a href="#2-2-2-优缺点" class="headerlink" title="2.2.2 优缺点"></a>2.2.2 优缺点</h3><p>优点：</p><ul><li>由于对每个用户和物品都抽象出Embedding来进行表示，因此泛化能力比协同过滤有所增强，并且对数据稀疏问题有一定缓解</li><li>空间复杂度显著降低，不再需要存储庞大的评分矩阵，只需要存储用户和物品的Embedding矩阵，由O(MN)降低到O((M+N)×k)</li><li>虽然前文一直在讲Embedding，但是这是为了方便表述和理解，请注意在以上几个方法提出来时，只有隐向量的概念，不过本质上矩阵分解就是为了产出Embedding，因此可以与现今的深度学习模型无缝衔接</li></ul><p> 缺点：</p><ul><li>以上几个经典的矩阵分解模型仍然没有考虑如何更好地引入用户、物品和各类上下文的有效信息，而SVD++也只是在这方面初步做出了尝试，矩阵分解系列算法的重心在于如何更好地根据评分矩阵分解出Embedding</li><li>由于基本上只考虑评分矩阵，因此无法对冷启动用户和物品进行有效推荐</li></ul><h2 id="2-3-逻辑回归"><a href="#2-3-逻辑回归" class="headerlink" title="2.3 逻辑回归"></a>2.3 逻辑回归</h2><p>逻辑回归是一个非常经典的机器学习模型，同时从深度学习的角度，也可以将其看作是一个单一神经元。相比于协同过滤和矩阵分解，逻辑回归将推荐问题看作一个分类问题，通过预测正样本的概率对物品进行排序，即将其转化为一个<strong>点击率（Click Through Rate，简称CTR）预估</strong>问题。</p><h3 id="2-3-1-数学形式"><a href="#2-3-1-数学形式" class="headerlink" title="2.3.1 数学形式"></a>2.3.1 数学形式</h3><p>逻辑回归本质上就是线性回归与Sigmoid函数的组合：<br><img src="/img/post/rec_traditional/lr.png"><br>其中，$x$是特征向量，$w^T$与$b$是可训练的参数。在推荐场景中，各种各样的特征，包括离散特征和连续特征，拼接在一起所形成的一个大向量即为$x$。</p><p>Sigmoid函数阈值在(0, 1)区间内，两边均为开区间，函数如下：<br><img src="/img/post/rec_traditional/sigmoid.jpg"><br>由于Sigmoid能将因变量映射到(0, 1)区间内，因此逻辑回归能够用来做二分类模型</p><h3 id="2-3-2-极大似然估计"><a href="#2-3-2-极大似然估计" class="headerlink" title="2.3.2 极大似然估计"></a>2.3.2 极大似然估计</h3><p>逻辑回归使用<strong>极大似然估计（Maximum Likelihood Estimate）</strong>的方法来训练各参数，极大似然估计是一种统计学中用来估计参数的方法。这里涉及到两个概念：<strong>概率</strong>和<strong>似然</strong>。概率，即已知规律，推导出某个事件理论上发生的可能性。似然，即已知结果，根据结果反推理论上各个事件的可能性分布。而极大似然估计即根据已知的样本信息，去反推出一套最有可能导致该结果的模型参数。这里省略逻辑回归中极大似然估计的推导过程，有兴趣可见该<a href="https://blog.csdn.net/zjuPeco/article/details/77165974">博客</a>。</p><h3 id="2-3-3-损失函数选择"><a href="#2-3-3-损失函数选择" class="headerlink" title="2.3.3 损失函数选择"></a>2.3.3 损失函数选择</h3><p>这个问题在面试中一般也很常见，也是逻辑回归与矩阵分解的一个很大的不同。矩阵分解中一般用<strong>平方损失（Squared Loss）</strong>函数，但是逻辑回归并不使用平方损失函数。如果使用平方损失函数，则会由于Sigmoid函数的存在，导致损失函数为非凸函数，这会使逻辑回归模型陷入局部最优的陷阱中。</p><p>逻辑回归使用<strong>对数似然损失（Log-likehood Loss）</strong>函数或<strong>交叉熵损失（Cross Entropy Loss）</strong>函数。在二分类场景下，这两者的表达式其实是一样的。但是对数似然损失只适用于二分类场景，即模型最后一层是Sigmoid函数，而交叉熵损失可以拓展到多分类场景，即模型最后一层是Softmax函数。从极大似然估计的角度出发，推导出来的损失函数即交叉熵损失函数，并且该函数是一个凸函数，方便使用梯度下降法进行学习和优化。</p><p>逻辑回归中的交叉熵损失函数如下：<br><img src="/img/post/rec_traditional/cross_entropy_loss.png"><br>在交叉熵损失函数基础上，逻辑回归使用梯度下降法来进行参数的更新优化。</p><h3 id="2-3-4-优缺点"><a href="#2-3-4-优缺点" class="headerlink" title="2.3.4 优缺点"></a>2.3.4 优缺点</h3><p>优点：</p><ul><li>逻辑回归假设因变量符合伯努利分布，而CTR问题的因变量也符合伯努利分布，因此在数学含义上是相符的</li><li>可解释性强，逻辑回归本质上是各特征的加权求和，因此查看各特征的权重，便可得知某物品被推荐的理由</li><li>模型简单有效，易于实现，训练开销小，而训练开销和在线推断又是工业界的一个痛点需求，在2012年GPU流行之前，逻辑回归都是工业界的主流模型</li></ul><p>缺点：</p><ul><li>性能极其依赖特征工程</li><li>逻辑回归本质上是个线性模型，结构简单，且不具备特征交叉能力</li></ul><h3 id="2-3-5-连续or离散"><a href="#2-3-5-连续or离散" class="headerlink" title="2.3.5 连续or离散"></a>2.3.5 连续or离散</h3><p>在工业界，一般很少将连续值直接输入到逻辑回归模型中，而是将其进行<strong>离散化</strong>，亦被称作<strong>特征分桶</strong>，转化成一系列0&#x2F;1的离散特征。这样做的优势在于：</p><ul><li>离散特征使模型可扩展性更好，添加和删除都十分方便，易于快速迭代</li><li>离散化后的特征鲁棒性很好，比如将大于30的年龄转化成1，否则是0，那么假如有一个异常数据300，如果直接作为连续值输入会给模型带来很大噪音</li><li>离散化后，相当于为模型在特征层面引入了非线性，能够提升模型的表达和拟合能力</li><li>离散化后的特征方便进行特征交叉，进一步提高模型的表达能力</li><li>许多连续特征，比如年龄，增减一岁区别不大，因此将其按区间离散化后可以使模型更稳定，同时也降低了模型过拟合的风险</li></ul><h3 id="2-3-6-逻辑回归的两个变种模型"><a href="#2-3-6-逻辑回归的两个变种模型" class="headerlink" title="2.3.6 逻辑回归的两个变种模型"></a>2.3.6 逻辑回归的两个变种模型</h3><h4 id="2-3-6-1-POLY2"><a href="#2-3-6-1-POLY2" class="headerlink" title="2.3.6.1 POLY2"></a>2.3.6.1 POLY2</h4><p>为了使逻辑回归能够拥有<strong>特征交叉</strong>的能力，一开始算法工程师采用人工特征交叉再筛选的方法，但是这种方式效率底下。为了能够从模型层面解决这个问题，POLY2被提了出来，公式如下：<br><img src="/img/post/rec_traditional/poly2.png"></p><p>这个公式描述的是线性组合部分，后面会接Sigmoid函数，以使预测值在(0, 1)区间内。公式的前两项本质上就是逻辑回归的线性组合部分，第三项为特征交叉部分。与逻辑回顾相同的是，在该公式所描述的推荐场景中，所有离散和连续特征拼接在一起形成了总的特征向量$x$，其维度为$n$。而公式中的$x_i$则表示特征向量$x$中第$i$维的值。因此初学者在初次接触POLY2，包括后面的FM、FFM等，会对公式中特征交叉的含义产生疑惑。在公式所描述的具体实现中，$x$中的每一维都是一个特征，特征交叉即是将$x$中的所有维度进行两两交叉，而非对宏观意义上的诸如性别与职业之类的特征进行交叉。</p><p>可以看到该模型对所有特征进行两两交叉，并对所有特征组合赋予权重$w_{ij}$，一定程度上解决了特征交叉的问题。并且POLY2虽然通过特征交叉引入了非线性，但是从模型层面来说仍然是线性模型，与逻辑回归并无二致，便于工程上的兼容。</p><p>但是POLY2也存在两个弊端：</p><ul><li>评分数据原本就是非常稀疏的，而POLY2中每个特征组合都有一个相应的权重系数，稀疏数据所对应的权重无法得到有效训练，使得数据更加稀疏</li><li>权重参数由$n$变为$n^2$，训练复杂度大大提升</li></ul><h4 id="2-3-6-2-LS-PLM"><a href="#2-3-6-2-LS-PLM" class="headerlink" title="2.3.6.2 LS-PLM"></a>2.3.6.2 LS-PLM</h4><p>LS-PLM（Large Scale Piece-wise Linear Model）又称MLR（Mixed Logistic Regression），由阿里巴巴于2017年公开，不过早在2012年就已经是阿里内部的主流推荐模型了。它在逻辑回归的基础之上，采用分而治之的思想，将样本进行分片，然后在样本分片中使用逻辑回归进行CTR预估。</p><p>LS-PLM的灵感来源于对广告推荐领域样本特点的观察。比如说，如果模型要预估女性受众点击女装广告的CTR，那么我们显然不希望把男性用户点击数码产品的样本也考虑进来，这些无关样本不仅与所考虑的场景毫无关系，并且会在模型训练过程中扰乱参数的训练。因此为了针对不同用户群体和不同场景，LS-PLM先对样本进行聚类，再对每类样本使用逻辑回归进行CTR预估。</p><p>LS-PLM的数学形式如下：<br><img src="/img/post/rec_traditional/mlr2.png"><br>其中$m$为分片数，即应该分为几种类别，能够较好地平衡模型的拟合和泛化能力，$m$越大，模型拟合能力越强，所需要的模型参数也越多。模型前者为一个Softmax公式，用来进行样本的多分类，后者为逻辑回归公式，用以进行CTR预估。该公式可以被抽象成如下形式：<br><img src="/img/post/rec_traditional/mlr.png"><br>从该形式中，我们已经能够看到注意力机制的雏形了。</p><p>LS-PLM通过分片的方式，自动挖掘数据中的非线性模式，省去大量的人工处理和特征工程的过程，可以端到端地进行训练。此外，在实际训练中，LS-PLM还引入了L1与L2,1范数（L2,1范数是一种特殊范数），这两种范数共同使得训练出来的模型具有较高的稀疏度，因此模型的部署更加轻量，在线推断速度也会更快。</p><h2 id="2-4-因子分解机"><a href="#2-4-因子分解机" class="headerlink" title="2.4 因子分解机"></a>2.4 因子分解机</h2><p>为了解决POLY2的弊端，同时引进特征交叉的强大功能，Rendle于2010年提出<strong>因子分解机（Factorization Machine，简称FM）</strong>。</p><h3 id="2-4-1-特征交叉"><a href="#2-4-1-特征交叉" class="headerlink" title="2.4.1 特征交叉"></a>2.4.1 特征交叉</h3><p>有时候，仅利用单一特征而非交叉特征进行判断时，不仅有可能造成信息损失的问题，还有可能导致错误的结论，这便是<strong>辛普森悖论</strong>。关于辛普森悖论的例子，可以谷歌或百度进一步了解。因此在推荐中，特征数目众多，对其进行特征交叉是非常有必要的。比如，对用户来说，我们有年龄、性别和职业三种特征，那么对年龄和性别进行交叉，我们便会得到一个新的交叉特征“年龄_性别”，其取值可以是比如“男_18”，“女_22”等等。</p><p>从one-hot的角度来考虑特征交叉可能会更容易理解。假如我们有职业、性别两个特征，各自取值区间分别在[“医生”，“警察”，“教师”]，[“男”，“女”]，则各自所对应的one-hot向量长度分别为3和2。将职业与性别进行交叉后所得到的“职业_性别”特征，其所对应的取值就有六种，one-hot向量维度也是6，将职业和性别的所有可能取值进行笛卡尔积，便可得到“职业_性别”特征的所有可能取值。</p><h3 id="2-4-2-模型结构"><a href="#2-4-2-模型结构" class="headerlink" title="2.4.2 模型结构"></a>2.4.2 模型结构</h3><p>FM结合了矩阵分解和POLY2的思想，模型中引入特征交叉，但是不为每个交叉特征都学习一个新的参数。取而代之的是，FM为每个特征引入一个隐向量，然后使用两个特征的隐向量的内积作为相应交叉特征的权重，公式如下：<br><img src="/img/post/rec_traditional/fm.png"></p><p>从公式中可以看出，与POLY2和逻辑回归相似的是，FM也是将原有的各个特征，比如职业、性别等，组合成一个总的特征向量$x$，然后将特征向量$x$中的每一维作为一个特征，对其进行特征交叉。因此FM是对每一维引入了一个隐向量，而非对诸如职业、性别等这样的特征引入了一个隐向量。此外，公式后面也会接一个Sigmoid用以将输出映射到(0,1)区间内。</p><h3 id="2-4-3-优缺点"><a href="#2-4-3-优缺点" class="headerlink" title="2.4.3 优缺点"></a>2.4.3 优缺点</h3><p>FM将矩阵分解的思想进行了扩展，从单纯地分解出用户和物品的隐向量，扩展到分解出每个特征的隐向量。此外隐向量的引入，极大地降低了模型复杂度，从POLY2的$n^2$降低到了$nk$（$k$为隐向量维度且$n$&gt;&gt;$k$）。并且由于FM的计算过程也进行了优化，其计算复杂度也被降低至$nk$，具体推导可参见这篇<a href="https://www.jianshu.com/p/25db169f11fa">博客</a>。</p><p>此外隐向量的引入能够让FM更好地处理数据稀疏的问题。因为交叉特征的权重是由两个单一特征的隐向量内积得来，因此只要其中某个特征的权重通过别的样本得到了训练，那么该交叉特征的权重就相当于进行了优化。而在POLY2模型中，一个样本必须同时满足两个单一特征取值不为0，其相应的交叉特征的权重才会被更新，但是一般情况下这种样本很少，导致交叉特征的权重参数很少被训练。此外，对于训练样本中未出现过的交叉特征，只要参与交叉的两个单一特征对应的隐向量被训练过，就可以通过内积计算该交叉特征的权重，因此FM的泛化能力也大大提高。</p><p>在工程方面，FM同样使用梯度下降法进行学习，并且模型容易实现，因此线上部署和推断也较为简单容易。在2012-2014年前后，成为业界主流推荐模型之一。</p><p>然而，FM一类模型并不能够引申到更高维度的特征交叉层面，因为三阶及以上的特征交叉会带来组合爆炸问题，导致模型复杂度和训练复杂度过高，这在实际应用中无法承受。</p><h3 id="2-4-4-FFM"><a href="#2-4-4-FFM" class="headerlink" title="2.4.4 FFM"></a>2.4.4 FFM</h3><p>FFM于2014年提出，并于2015年在多项CTR预估大赛中夺得冠军。FFM在FM基础上，为了进一步增强模型的学习能力，引入了<strong>特征域感知（field-aware）</strong>的概念。在理解了POLY2和FM的实际实现后，FFM的原理其实非常好懂。POLY2和FM的表述提到了两种不同的特征，一种是我们常说的比较宏观、易于理解的特征，比如性别、职业、年龄等，一种是在具体实现时所涉及的、相对更微观的特征，即将特征向量$x$的每一维作为一个特征。而FFM则是将前一种称为特征域（field），然后在引入隐向量的时候，人为考虑了不同特征域的不同影响。具体计算公式如下：<br><img src="/img/post/rec_traditional/ffm.png"><br>可以看到，隐向量变成了$\mathbf{V}_{i,f_j}$，而非FM中的$\mathbf{V}_i$，这意味着每个特征对应着一组隐向量。当特征$x_i$与$x_j$进行交叉时，$x_i$会挑出与$x_j$的域$f_j$对应的隐向量$\mathbf{V}_{i,f_j}$进行交叉，$x_j$这边也是同理。</p><p>由于FFM的计算复杂度不可被优化，因此为$kn^2$。模型复杂度为$nkf$，$n$为特征个数，$k$为隐向量维度，$f$为域的数量。</p><p>与FM相比，FFM引入特征域的概念，使得模型的表达能力更强，但是计算复杂度也有所上升，在实际应用中，需要对二者进行权衡。</p><h2 id="2-5-GBDT-LR"><a href="#2-5-GBDT-LR" class="headerlink" title="2.5 GBDT+LR"></a>2.5 GBDT+LR</h2><p>FM系列模型只能处理二阶特征交叉，如果继续提高特征交叉的维度，会产生组合爆炸与计算复杂度过高的问题。因此在2014年，Facebook提出了基于GBDT+LR的组合模型来有效地进行<strong>高维特征组合和筛选</strong>。在该组合中，特征首先会通过GBDT自动进行筛选和组合，生成新的离散特征向量，然后该特征向量会输入到LR中做CTR预估。在这个过程中，GBDT用来自动进行特征工程，LR用来进行CTR预估，这两部分是独立训练的，因此无需考虑如何将LR中的梯度传播到GBDT中这种比较棘手的问题。</p><h3 id="2-5-1-GBDT原理"><a href="#2-5-1-GBDT原理" class="headerlink" title="2.5.1 GBDT原理"></a>2.5.1 GBDT原理</h3><p>GBDT（Gradient Boosting Decision Tree，梯度提升决策树）是集成学习boosting类别中的经典模型，由多颗决策树组成。每一颗树以前面树林的结果与真实标签的残差作为拟合目标，其生成过程是一颗标准的回归树的生成过程，因此每个节点的分裂都是一个自然的特征选择的过程。而多层节点则对特征进行了有效的自动组合，因此高效地解决了特征选择和特征组合的问题。</p><h3 id="2-5-2-如何进行特征组合"><a href="#2-5-2-如何进行特征组合" class="headerlink" title="2.5.2 如何进行特征组合"></a>2.5.2 如何进行特征组合</h3><p>首先使用训练集对GBDT进行训练。训练过程中，GBDT预测输出的并不是一个二分类概率值。对于每一个训练样本，其在每一颗树中最终落到的叶子节点会被设置为1，其余叶子节点会被设置为0，所有树的叶子节点组合在一起便形成了一个用以输入到LR模型的离散型特征向量。</p><p>在这个过程中，GBDT中决策树的深度决定了特征组合的阶数。假如深度为4，则通过3次节点分裂，最终的叶子节点实际上是进行三阶特征组合后的结果。</p><h3 id="2-5-3-优缺点"><a href="#2-5-3-优缺点" class="headerlink" title="2.5.3 优缺点"></a>2.5.3 优缺点</h3><p>GBDT+LR让模型自动完成特征工程，使得算法工程师不必在特征工程上花费过多精力，实现了真正的<strong>端到端（End2End）</strong>的训练。而在这之前，特征工程的主要方法有两个，一个是人工特征组合和筛选，一个是改进目标结构、损失函数或增加特征交叉项。前者对工程师的经验和精力要求较高，后者对模型设计能力要求较高。</p><p>但是GBDT无法进行完全并行的训练，训练时间较长，并且GBDT自身没有诸如正则化项之类的防止过拟合的有效手段，因此容易出现过拟合。并且虽然GBDT理论上能够进行高阶的特征组合，但是GBDT的特征组合过程丢失了大量的数值信息，因此GBDT并不一定就比FM系列要好。</p><h1 id="3-结语"><a href="#3-结语" class="headerlink" title="3 结语"></a>3 结语</h1><p>在深度学习大行其道的今天，各种模型五花八门，从今天回望过去，以上所介绍的模型可能过于单一和乏味。尽管如此，这些模型仍然值得回味和学习，其中所蕴含的思想仍然指导着如今大多数推荐模型的演化和发展，这大概就是经典永流传吧。在后续的文章中，我将进一步整理深度学习时代的经典模型，希望能够坚持把这个系列整理完。</p><p>参考文献：</p><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1423975">Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://zh.wikipedia.org/wiki/%E5%8D%94%E5%90%8C%E9%81%8E%E6%BF%BE">协同过滤维基百科</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>recommendation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>recommendation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>推荐之框架理解</title>
    <link href="/posts/2110556878/"/>
    <url>/posts/2110556878/</url>
    
    <content type="html"><![CDATA[<p><a href="https://www.zhihu.com/question/314773668">待整理</a></p>]]></content>
    
    
    <categories>
      
      <category>recommendation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>recommendation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch理解</title>
    <link href="/posts/1090588111/"/>
    <url>/posts/1090588111/</url>
    
    <content type="html"><![CDATA[<h1 id="Tensor-存储"><a href="#Tensor-存储" class="headerlink" title="Tensor 存储"></a>Tensor 存储</h1><p><a href="https://blog.csdn.net/Flag_ing/article/details/109129752">https://blog.csdn.net/Flag_ing/article/details/109129752</a></p><h1 id="nn-Parameter-nn-Linear-nn-Embedding-三者的区别"><a href="#nn-Parameter-nn-Linear-nn-Embedding-三者的区别" class="headerlink" title="nn.Parameter, nn.Linear, nn.Embedding 三者的区别"></a>nn.Parameter, nn.Linear, nn.Embedding 三者的区别</h1><p><a href="https://audreywongkg.medium.com/pytorch-nn-parameter-vs-nn-linear-2131e319e463">https://audreywongkg.medium.com/pytorch-nn-parameter-vs-nn-linear-2131e319e463</a></p>]]></content>
    
    
    <categories>
      
      <category>deep learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>deep learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>scipy踩坑</title>
    <link href="/posts/368754817/"/>
    <url>/posts/368754817/</url>
    
    <content type="html"><![CDATA[<!-- toc --><h1 id="1-稀疏矩阵"><a href="#1-稀疏矩阵" class="headerlink" title="1 稀疏矩阵"></a>1 稀疏矩阵</h1><p><code>scipy.sparse</code>库中提供了多种稀疏矩阵，<code>coo_matrix</code>、<code>csr_matrix</code>、<code>csc_matrix</code>、<code>bsr_matrix</code>、<code>dia_matrix</code>、<code>dok_matrix</code>与<code>lil_matrix</code>，本节整理前三种最常用的稀疏矩阵。</p><h2 id="1-1-常用API"><a href="#1-1-常用API" class="headerlink" title="1.1 常用API"></a>1.1 常用API</h2><ul><li>mat.todense()：转成numpy.matrix</li><li>mat.toarray()：转成numpy.ndarray</li><li>mat.transpose()：转置</li><li>mat.getnnz()：获取非零元素个数</li><li>mat.A：与mat.toarray()相同</li><li>mat.T：与mat.transpose()相同</li><li>mat.size：与mat.getnnz()相同</li><li>mat.shape：矩阵形状</li></ul><h2 id="1-2-coo-matrix"><a href="#1-2-coo-matrix" class="headerlink" title="1.2 coo_matrix"></a>1.2 coo_matrix</h2><h3 id="1-2-1-矩阵创建"><a href="#1-2-1-矩阵创建" class="headerlink" title="1.2.1 矩阵创建"></a>1.2.1 矩阵创建</h3><p>coo_matrix使用三元组<code>(data, (row, col))</code>描述非零元素，分别对应元素的行、列和值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scipy.sparse <span class="hljs-keyword">as</span> sp<br><br>row = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]<br>col = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]<br>data = [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]<br><br>mat = sp.coo_matrix((data, (row, col)), shape=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), dtype=np.int32)<br></code></pre></td></tr></table></figure><h3 id="1-2-2-使用场景"><a href="#1-2-2-使用场景" class="headerlink" title="1.2.2 使用场景"></a>1.2.2 使用场景</h3><p>coo_matrix采用坐标形式存储矩阵，主要用来快速地<strong>构建矩阵</strong>，可以将numpy、pandas等读取的数据快速转为稀疏矩阵，创建后可转为其他类型的矩阵进行转置、矩阵乘等操作。<br>coo_matrix能够非常高效快速地转为csr_matrix和csc_matrix，也能灵活方便地转成其他稀疏矩阵（<code>mat.tocsr()</code>），允许同一位置有重复项（转换成csr_matrix与csc_matrix时同一位置的元素会相加）。但是coo_matrix无法对元素进行增删改，也没有切片和矩阵算术操作。</p><h3 id="1-2-3-coo-matrix属性："><a href="#1-2-3-coo-matrix属性：" class="headerlink" title="1.2.3 coo_matrix属性："></a>1.2.3 coo_matrix属性：</h3><ul><li>dtype：矩阵元素的值类型</li><li>shape：矩阵形状</li><li>ndim：矩阵维度，一般为2</li><li>nnz： 存储值的个数，包括显示声明的零元素</li><li>data：元素的值</li><li>row：元素的行</li><li>col：元素的列</li></ul><h2 id="1-3-csr-matrix与csc-matrix"><a href="#1-3-csr-matrix与csc-matrix" class="headerlink" title="1.3 csr_matrix与csc_matrix"></a>1.3 csr_matrix与csc_matrix</h2><h3 id="1-3-1-矩阵创建"><a href="#1-3-1-矩阵创建" class="headerlink" title="1.3.1 矩阵创建"></a>1.3.1 矩阵创建</h3><p>csr_matrix使用三元组<code>(data, indices, indptr)</code>或<code>(data, (row, col))</code>描述非零元素，indices和indptr分别表示每个元素的列和行偏移量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scipy.sparse <span class="hljs-keyword">as</span> sp<br><br>indptr = [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]<br>indices = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>]<br>data = [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]<br><br>mat = sp.coo_matrix((data, indices, indptr), shape=(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>), dtype=np.int32)<br></code></pre></td></tr></table></figure><p>在上例中，indptr的含义是indices和data中第[0, 2)的值属于第0行，第[2, 4)的值属于第1行，以此类推。因此indptr的长度等于行数+1，而indices和data的长度与非零值数量一致。</p><h3 id="1-3-2-使用场景"><a href="#1-3-2-使用场景" class="headerlink" title="1.3.2 使用场景"></a>1.3.2 使用场景</h3><p>csr_matrix采用按行压缩形式存储矩阵，主要用于<strong>实际计算</strong>。<br>csr_matrix，能高效地按行切片，能进行快速的矩阵向量相乘，还能进行高效的CSR+CSR、CSR*CSR等其他算术操作。但是csr_matrix的列切片相比csc_matrix较慢，并且稀疏结构的变化开销比较大（可以考虑LIL或DOK）。</p><h3 id="1-3-3-csr-matrix属性："><a href="#1-3-3-csr-matrix属性：" class="headerlink" title="1.3.3 csr_matrix属性："></a>1.3.3 csr_matrix属性：</h3><ul><li>dtype：矩阵元素的值类型</li><li>shape：矩阵形状</li><li>ndim：矩阵维度，一般为2</li><li>nnz： 存储值的个数，包括显示声明的零元素</li><li>data：元素的值</li><li>indices：元素的列</li><li>indptr：元素的行偏移量</li><li>has_sorted_indices：indices是否已排序</li></ul><p>csc_matrix则与csr_matrix相似，是按列压缩进行矩阵存储的，这里不再赘述。</p><h2 id="1-4-矩阵形式转换"><a href="#1-4-矩阵形式转换" class="headerlink" title="1.4 矩阵形式转换"></a>1.4 矩阵形式转换</h2><h3 id="1-4-1-scipy-sparse内互转"><a href="#1-4-1-scipy-sparse内互转" class="headerlink" title="1.4.1 scipy.sparse内互转"></a>1.4.1 scipy.sparse内互转</h3><p>任何稀疏矩阵都可以通过<code>mat.tocoo()</code>方法转换成coo_matrix，coo_matrix也可以通过<code>mat.tocsr()</code>、<code>mat.tocsc()</code>等方法转换成其他稀疏矩阵。</p><h3 id="1-4-2-scipy-sparse与numpy互转"><a href="#1-4-2-scipy-sparse与numpy互转" class="headerlink" title="1.4.2 scipy.sparse与numpy互转"></a>1.4.2 scipy.sparse与numpy互转</h3><p>numpy.ndarray和numpy.matrix可以直接传入scipy稀疏矩阵的构造函数中来构建稀疏矩阵。<br>scipy稀疏矩阵可以通过<code>mat.todense()</code>方法转成numpy.matrix，通过<code>mat.toarray()</code>方法转成numpy.ndarray。</p><h3 id="1-4-3-scipy-sparse转成torch-sparse"><a href="#1-4-3-scipy-sparse转成torch-sparse" class="headerlink" title="1.4.3 scipy.sparse转成torch.sparse"></a>1.4.3 scipy.sparse转成torch.sparse</h3><p>从scipy.sparse中的稀疏矩阵形式转成torch.sparse中的稀疏矩阵形式，需要借助numpy，以下为一个 示例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sp_matrix_to_ts_tensor</span>(<span class="hljs-params">matrix</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    change a coo matrix to a torch sparse tensor</span><br><span class="hljs-string">    matrix: scipy.sparse.coo_matrix或numpy.ndarray</span><br><span class="hljs-string">    return: torch.sparse.tensor</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    matrix = matrix.tocoo()<br>    indices = torch.from_numpy(np.vstack((matrix.row, matrix.col)).astype(np.int64))<br>    values = torch.from_numpy(matrix.data).<span class="hljs-built_in">float</span>()<br>    <span class="hljs-keyword">return</span> torch.sparse.FloatTensor(indices, values, matrix.shape)<br></code></pre></td></tr></table></figure><h2 id="1-5-其他类型的稀疏矩阵"><a href="#1-5-其他类型的稀疏矩阵" class="headerlink" title="1.5 其他类型的稀疏矩阵"></a>1.5 其他类型的稀疏矩阵</h2><ul><li><a href="https://www.biaodianfu.com/scipy-sparse.html">SciPy稀疏矩阵模块scipy.sparse</a></li><li><a href="https://wizardforcel.gitbooks.io/scipy-lecture-notes/content/10.html">scipy lecture notes中文版</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>data process</category>
      
    </categories>
    
    
    <tags>
      
      <tag>scipy</tag>
      
      <tag>data process</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>理解和使用numpy</title>
    <link href="/posts/130345454/"/>
    <url>/posts/130345454/</url>
    
    <content type="html"><![CDATA[<!-- toc --><h1 id="1-numpy基本理解"><a href="#1-numpy基本理解" class="headerlink" title="1 numpy基本理解"></a>1 numpy基本理解</h1><p>numpy库是现在python生态中最广为使用的基础数学计算工具库，能够用来存储和计算大型矩阵和多维度数组，它针对数组计算提供了大量的数学函数库，相比于python原生list，numpy计算在内存和时间上的开销都要小很多。</p><p>numpy基本上与python生态中其他所有主流的数据处理和机器学习工具库都有联系，如pandas、scipy、sklearn和pytorch等深度学习框架，这些工具库或者框架都拥有一套自己的数据结构，而这些数据结构大多是在numpy基础上进行扩展，能够兼容numpy并且都能跟numpy互相转化。</p><p>numpy中的基本元素是ndarray（n dimensional array，n维数组，也称为一个array），ndarray可以将同一类型的元素存储成多个维度，其中每一维度被称为一个axis，并有一定的长度length。如ndarray<code>[[0., 1., 2.],[3., 4., 5.]]</code>有2个维度，长度分别为2和3。一个ndarray含有如下常用API：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; import numpy as np</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a = np.array([[1,2,3],[4,5,6]])</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a.ndim  <span class="hljs-comment"># 有几个维度</span></span><br>2<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a.shape  <span class="hljs-comment"># 有几个维度，每个维度长度多少</span></span><br>(2, 3)<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a.size  <span class="hljs-comment"># 元素数量</span></span><br>6<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a.dtype  <span class="hljs-comment"># 元素类型</span></span><br>dtype(&#x27;int32&#x27;)<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a.itemsize  <span class="hljs-comment"># 元素占几个字节</span></span><br>4<br></code></pre></td></tr></table></figure><h1 id="2-使用ndarray"><a href="#2-使用ndarray" class="headerlink" title="2 使用ndarray"></a>2 使用ndarray</h1><h2 id="2-1-创建ndarray"><a href="#2-1-创建ndarray" class="headerlink" title="2.1 创建ndarray"></a>2.1 创建ndarray</h2><p>numpy提供了多种创建ndarray的方法。<br><code>np.array()</code>是最基本的方法，接受一个list作为参数，元素类型与list中的元素类型一致。如果list中有多个元素类型，则按照complex&gt;float&gt;int&gt;bool的优先级进行类型的转化（True和False会分别转化成1.和0.）<br>除了np.array，numpy还提供了多种创建ndarray的API，这些API接受一个shape或者ndarray作为参数，数据类型默认为float64，可以在构造函数中用dtype指定类型常用API如下:</p><ul><li><code>np.zeros()</code>，<code>np.zeros_like()</code>：元素全为0</li><li><code>np.ones()</code>，<code>np.ones_like()</code>：元素全为1</li><li><code>np.empty()</code>，<code>np.empty_like()</code>：随机初始化元素，值取决于内存状态</li><li><code>np.arange()</code>，<code>np.linspace()</code>：创建一个序列，前者可以指定两个元素间隔的值，后者指定序列长度</li><li><code>np.eye()</code>：创建对角矩阵</li><li><code>np.diag()</code>：创建对角矩阵或者提取对角线的元素形成一个ndarray</li></ul><h2 id="2-2-数据类型"><a href="#2-2-数据类型" class="headerlink" title="2.2 数据类型"></a>2.2 数据类型</h2><p>numpy所支持的数据类型要比python内置类型多很多，基本可以与C的数据类型对应，常用类型如下：</p><ul><li>bool：布尔类型</li><li>int8，int16，int32，int64：有符号整数，后面的值表示占多少字节</li><li>uint8，uint16，uint32，uint64：无符号整数，后面的值表示占多少字节</li><li>float16，float32，float64，float128：分别是半精度、单精度（float）、双精度（double）和扩展精度的浮点数</li><li>object：python对象</li><li>complex64，complex128，complex256：分别由两个float32、float64、float128描述的复数</li></ul><p>在创建ndarray的时候，即可以通过赋值<code>dtype</code>来指定ndarray的数据类型。也可以在运行时，使用<code>astype()</code>方法来改变一个ndarray的数据类型，不过该方法并不会直接修改原数据类型。</p><h2 id="2-3-打印"><a href="#2-3-打印" class="headerlink" title="2.3 打印"></a>2.3 打印</h2><p>一般用<code>print()</code>就可以打印一个ndarray，不过ndarray过大的话，只会显示部分元素，这时可以使用<code>np.set_printoptions(threshold=sys.maxsize)</code>强制打印完全。</p><h2 id="2-4-数学运算"><a href="#2-4-数学运算" class="headerlink" title="2.4 数学运算"></a>2.4 数学运算</h2><p>在numpy中基本的数学运算是element-wise的，并且会创建一个新的ndarray。如加（<code>+</code>）、减（<code>-</code>）、乘（<code>*</code>）、幂（<code>**</code>）、比较符（<code>&lt;</code>等）。特别注意，矩阵乘法使用<code>@</code>或者<code>ndarray.dot()</code>。<br>而如<code>+=</code>、<code>*=</code>这种操作符，会直接改变原有ndarray的值，而非创建一个新的ndarray，即操作是in-place的。<br>同<code>np.array()</code>一样，如果运算中碰到了多种不同类型的元素，则会按照complex&gt;float&gt;int&gt;bool的优先级进行类型的转化。</p><p>以上运算都是涉及到多个ndarray的多元运算，在调用API的时候一般是以np为句柄。只涉及到一个ndarray的一元运算或者操作，则句柄基本都是ndarray本身，并且运算会作用到ndarray中每一个元素上，如<code>sum()</code>，<code>min()</code>，<code>max()</code>，<code>mean()</code>，<code>exp()</code>，<code>sqrt()</code>，<code>sin()</code>，<code>cos()</code>，<code>argmax()</code>，<code>argmin()</code>，<code>sort()</code>，<code>floor()</code>，<code>ceil()</code>等。</p><h2 id="2-5-索引、切片"><a href="#2-5-索引、切片" class="headerlink" title="2.5 索引、切片"></a>2.5 索引、切片</h2><h3 id="2-5-1-基本的索引和切片操作"><a href="#2-5-1-基本的索引和切片操作" class="headerlink" title="2.5.1 基本的索引和切片操作"></a>2.5.1 基本的索引和切片操作</h3><p>对于一维ndarray，其索引方式与python中的list完全相同。</p><p>对于多维ndarray，也可以像多维list那样进行索引或者切片，但是更常用的一种方式是传入一个tuple对所有维进行索引或者切片：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; import numpy as np</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a = np.array([[1,2,3],[4,5,6]])</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a[1][0]  <span class="hljs-comment"># 按照list的方式索引</span></span><br>4<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a[1, 0]  <span class="hljs-comment"># 推荐使用的索引方式</span></span><br>4<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a[0, :2]  <span class="hljs-comment"># 切片</span></span><br>array([1, 2])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a[:1]  <span class="hljs-comment"># 如果后面的维度被省略，则选取所有的元素</span></span><br>array([[1, 2, 3]])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a[..., :2]  <span class="hljs-comment"># 如果想省略前面的维度，则使用...</span></span><br>array([[1, 2],<br>       [4, 5]])<br></code></pre></td></tr></table></figure><p>索引的时候可以指定newaxis（其值实际为None）来扩展一个新的维度：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; import numpy as np</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; from numpy import newaxis</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a = np.array([[1,2,3],[4,5,6]])</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a[newaxis]</span><br>array([[[1, 2, 3],<br>        [4, 5, 6]]])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a[newaxis].shape</span><br>(1, 2, 3)<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a[None].shape</span><br>(1, 2, 3)<br></code></pre></td></tr></table></figure><h3 id="2-5-2-使用数组索引"><a href="#2-5-2-使用数组索引" class="headerlink" title="2.5.2 使用数组索引"></a>2.5.2 使用数组索引</h3><p>除了通过切片为每个维度划定一个范围进行索引外，有时候我们还希望能够直接按下标索引到一批元素，这时候可以通过数组进行索引：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a = np.arange(10)</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; i = np.array([0,0,3,5,9])</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a[i]</span><br>array([0, 0, 3, 5, 9])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; i = np.array([[2, 3], [4, 5]])</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a[i]</span><br>array([[2, 3],<br>       [4, 5]])<br></code></pre></td></tr></table></figure><p>如果被索引的ndarray是多维的，进行索引的ndarray只有一个，那么它只能索引索引到原ndarray的第一个维度：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a = np.arange(10).reshape(5, 2)</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a</span><br>array([[0, 1],<br>       [2, 3],<br>       [4, 5],<br>       [6, 7],<br>       [8, 9]])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; i = np.array([[0, 4], [1, 2]])</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a[i]</span><br>array([[[0, 1],<br>        [8, 9]],<br><br>       [[2, 3],<br>        [4, 5]]])<br></code></pre></td></tr></table></figure><p>因此如果对多维ndarray进行索引，那么需要使用多个用来索引的ndarray：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; j = np.array([[0, 1],[1, 1]])</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a[i, j]</span><br>array([[0, 9],<br>       [3, 5]])<br></code></pre></td></tr></table></figure><h3 id="2-5-3-mask索引"><a href="#2-5-3-mask索引" class="headerlink" title="2.5.3 mask索引"></a>2.5.3 mask索引</h3><p>在机器学习中，还有一种常用的索引方式是mask索引，即通过True和False来索引相应的元素：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a = np.arange(12).reshape(3, 4)</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a</span><br>array([[ 0,  1,  2,  3],<br>       [ 4,  5,  6,  7],<br>       [ 8,  9, 10, 11]])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; b = a &gt; 4</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a[b]</span><br>array([ 5,  6,  7,  8,  9, 10, 11])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a[b] = 0</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a</span><br>array([[0, 1, 2, 3],<br>       [4, 0, 0, 0],<br>       [0, 0, 0, 0]])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a = np.arange(12).reshape(3,4)</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; np.where(a&gt;4, 0, 1)  <span class="hljs-comment"># 使用np.where进行索引并赋新值</span></span><br>array([[1, 1, 1, 1],<br>       [1, 0, 0, 0],<br>       [0, 0, 0, 0]])<br></code></pre></td></tr></table></figure><h3 id="2-5-4-其他有关方法"><a href="#2-5-4-其他有关方法" class="headerlink" title="2.5.4 其他有关方法"></a>2.5.4 其他有关方法</h3><p>numpy提供了一些API能够根据不同的条件来提取符合条件的元素下标：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a = np.arange(12).reshape(3, 4)</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a</span><br>array([[ 0,  1,  2,  3],<br>       [ 4,  5,  6,  7],<br>       [ 8,  9, 10, 11]])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a.nonzero()  <span class="hljs-comment"># 获取非0元素的下标</span></span><br>(array([0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2], dtype=int64), array([1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3], dtype=int64))<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; np.where(a&gt;3)  <span class="hljs-comment"># 获取符合条件的元素的下标</span></span><br>(array([1, 1, 1, 1, 2, 2, 2, 2], dtype=int64), array([0, 1, 2, 3, 0, 1, 2, 3], dtype=int64))<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; np.where(a&gt;3, 0, 1)  <span class="hljs-comment"># 依据条件进行修改</span></span><br>array([[1, 1, 1, 1],<br>       [0, 0, 0, 0],<br>       [0, 0, 0, 0]])<br></code></pre></td></tr></table></figure><h2 id="2-6-shape相关"><a href="#2-6-shape相关" class="headerlink" title="2.6 shape相关"></a>2.6 shape相关</h2><h3 id="2-6-1-改变shape"><a href="#2-6-1-改变shape" class="headerlink" title="2.6.1 改变shape"></a>2.6.1 改变shape</h3><p>一个ndarray的shape由<code>shape</code>返回，可以通过<code>reshape()</code>、<code>ravel()</code>、<code>transpose()</code>操作改变ndarray的shape，不过这种改变不是in-place的，而是返回了一个新的指定shape的ndarray，如果要在原有ndarray上改变shape则需要<code>resize()</code>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; import numpy as np</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a = np.array([[1,2,3],[4,5,6]])</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a.reshape(3, 2)  <span class="hljs-comment"># 返回一个新的ndarray</span></span><br>array([[1, 2],<br>       [3, 4],<br>       [5, 6]])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a.ravel()  <span class="hljs-comment"># 返回一个新的一维ndarray</span></span><br>array([1, 2, 3, 4, 5, 6])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a.T  <span class="hljs-comment"># 转置</span></span><br>array([[1, 4],<br>       [2, 5],<br>       [3, 6]])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a.transpose()  <span class="hljs-comment"># 转置</span></span><br>array([[1, 4],<br>       [2, 5],<br>       [3, 6]])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a.resize(3, 2)  <span class="hljs-comment"># 修改原有ndarray</span></span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a</span><br>array([[1, 2],<br>       [3, 4],<br>       [5, 6]])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a.reshape(2, -1)  <span class="hljs-comment"># -1表示该维度的大小由numpy自动计算</span></span><br>array([[1, 2, 3],<br>       [4, 5, 6]])<br></code></pre></td></tr></table></figure><h3 id="2-6-2-拼接多个narray"><a href="#2-6-2-拼接多个narray" class="headerlink" title="2.6.2 拼接多个narray"></a>2.6.2 拼接多个narray</h3><p>最灵活的方法是<code>np.concatenate()</code>，可以将多个ndarray按照指定axis进行拼接。<code>np.vstack()</code>和<code>np.hstack()</code>则是分别将多个ndarray在一维和二维上进行拼接，被拼接的ndarray可以是一维、二维或者多维的。考虑二维的情形，v即vertical，h即horizontal。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">import numpy as np<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a = np.zeros((3, 4))</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; b = np.ones((3, 4))</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; np.concatenate((a, b), axis=0)  <span class="hljs-comment"># 在指定维度上进行拼接</span></span><br>array([[0., 0., 0., 0.],<br>       [0., 0., 0., 0.],<br>       [0., 0., 0., 0.],<br>       [1., 1., 1., 1.],<br>       [1., 1., 1., 1.],<br>       [1., 1., 1., 1.]])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; np.vstack((a, b))  <span class="hljs-comment"># 在一维上进行拼接</span></span><br>array([[0., 0., 0., 0.],<br>       [0., 0., 0., 0.],<br>       [0., 0., 0., 0.],<br>       [1., 1., 1., 1.],<br>       [1., 1., 1., 1.],<br>       [1., 1., 1., 1.]])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; np.hstack((a, b))  <span class="hljs-comment"># 在二维上进行拼接</span></span><br>array([[0., 0., 0., 0., 1., 1., 1., 1.],<br>       [0., 0., 0., 0., 1., 1., 1., 1.],<br>       [0., 0., 0., 0., 1., 1., 1., 1.]])<br></code></pre></td></tr></table></figure><h3 id="2-6-3-拆分一个ndarray"><a href="#2-6-3-拆分一个ndarray" class="headerlink" title="2.6.3 拆分一个ndarray"></a>2.6.3 拆分一个ndarray</h3><p>与拼接类似，拆分也有三种操作，分别对应于<code>array_split()</code>，<code>vsplit()</code>，<code>hsplit()</code>，这里不再赘述。</p><h1 id="3-numpy的主要机制"><a href="#3-numpy的主要机制" class="headerlink" title="3 numpy的主要机制"></a>3 numpy的主要机制</h1><h2 id="3-1-拷贝机制"><a href="#3-1-拷贝机制" class="headerlink" title="3.1 拷贝机制"></a>3.1 拷贝机制</h2><h3 id="3-1-1-ndarray的内存结构"><a href="#3-1-1-ndarray的内存结构" class="headerlink" title="3.1.1 ndarray的内存结构"></a>3.1.1 ndarray的内存结构</h3><p>一个ndarray对象封装了如下信息：</p><ul><li>dtype：数据类型</li><li>ndim：有几个维度</li><li>shape：ndarray各维度的长度</li><li>strides：维间距，在当前维到达下一个连续的元素所需要前进的字节数</li><li>data：指针，指向原始数据的内存区域</li></ul><p>这里借鉴一个网图来具体展示ndarray的内存结构：<br><img src="/img/post/numpy/ndarray_memory.png" alt="ndarray内存结构"></p><p>以上所有的数据都被称为元数据，也是一个ndarray实际所包含的信息。由data一项可以看到一个ndarray对象和原始数据是分开的，在只考虑data的情况下，ndarray对象本身相当于指针或者引用，而它所指向的东西即为存储的原始数据。因此实际的数据分布和我们看到的数据分布是不一样的，我们在对数据进行reshape这导致在numpy中进行拷贝会涉及多种情况。</p><h3 id="3-1-2-ndarray中拷贝的三种情况"><a href="#3-1-2-ndarray中拷贝的三种情况" class="headerlink" title="3.1.2 ndarray中拷贝的三种情况"></a>3.1.2 ndarray中拷贝的三种情况</h3><p>numpy中的引用实际上应该是一个多级关系：变量名-&gt;ndarray对象-&gt;数据，因此拷贝应该存在如下三种情况：</p><ul><li>赋值操作<code>=</code>不会对ndarray对象进行任何的复制，不会进行任何对象的创建，即使<code>=</code>之后我们有两个变量名，他们本质上是完全一样的。从python语言角度考虑，变量名本身也是一种引用，因此这其实属于浅拷贝。</li><li><code>view()</code>操作实际上是返回了一个新的ndarray对象，但是新的ndarray与原来的ndarray共享同一份原始数据。这其实也是浅拷贝，只不过跟<code>=</code>的区别是两个变量名所指向的ndarray对象不一样，但是数据本身是一样的，因此是浅拷贝。一个ndarray的数据发生改变后，另一个ndarray的数据也会随之变化。同样地，在用切片或<code>reshape()</code>等操作时，也会返回一个ndarray对象的浅拷贝，虽然看着是一个新数组，但是数据实际是同一份数据。</li><li>而<code>copy()</code>则好理解很多，它会同时拷贝ndarray对象和实际的数据，是深拷贝，两个变量名指向的是不同的ndarray对象，存储的也是两份不同的数据，一个ndarray发生变化，另一个不会发生变化。</li></ul><p>numpy官方文档里将上述三种情况分别称为“No Copy at All”、“View or Shallow Copy”和“Deep Copy”，官方文档的相关叙述可见<a href="https://numpy.org/devdocs/user/quickstart.html#copies-and-views">这里</a>。</p><h2 id="3-2-广播机制"><a href="#3-2-广播机制" class="headerlink" title="3.2 广播机制"></a>3.2 广播机制</h2><p>在实际应用中，常常会碰到shape不一样的ndarray参与一个element-wise的运算，这种运算实际上是有意义的，为了使不同shape的ndarray能够直接进行运算，需要用到广播机制。</p><p>同拷贝机制一样，广播机制也是numpy中重要的机制之一，并且都被pytorch等框架所继承和采纳。广播机制有如下两个原则：</p><ul><li>如果参与运算的ndarray的shape不一致，则维数比较少的ndarray会反复添加一个长度为1的维度，直到两者拥有一样多的维度；</li><li>如果有的维度长度不一样且某些ndarray在该维度上的长度为1，则这些ndarray会在长度为1的维度上进行复制，并与最长的维度保持一致。</li></ul><p>广播机制完成后，所有参与运算的ndarray的shape一定是相同的。以下是一个简单的示例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; import numpy as np</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a = np.array([[0,0,0],[1,1,1],[2,2,2]])</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; b = np.array([1,2,3])  <span class="hljs-comment"># 第一个原则</span></span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a</span><br>array([[0, 0, 0],<br>       [1, 1, 1],<br>       [2, 2, 2]])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; b</span><br>array([1, 2, 3])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a + b  <span class="hljs-comment"># 第一个原则</span></span><br>array([[1, 2, 3],<br>       [2, 3, 4],<br>       [3, 4, 5]])<br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; b = np.array([[1],[2],[3]])  <span class="hljs-comment"># 第二个原则</span></span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; a + b  <span class="hljs-comment"># 第二个原则</span></span><br>array([[1, 1, 1],<br>       [3, 3, 3],<br>       [5, 5, 5]])<br></code></pre></td></tr></table></figure><h1 id="4-ndarray与matrix"><a href="#4-ndarray与matrix" class="headerlink" title="4 ndarray与matrix"></a>4 ndarray与matrix</h1><p>ndarray可以是多维的，而matrix则如它的名字一样只能是个二维的矩阵，可以通过<code>np.mat()</code>方法进行构建。matrix继承自ndarray，因此会有很多相通的方法和属性。</p><h2 id="4-1-两者的不同"><a href="#4-1-两者的不同" class="headerlink" title="4.1 两者的不同"></a>4.1 两者的不同</h2><p>在使用上，两者的区别主要体现在矩阵乘法上。两者的<code>dot()</code>都是矩阵乘法，<code>multiply()</code>都是逐元素相乘，但是matrix的<code>*</code>表示矩阵相乘，而ndarray的<code>*</code>表示逐元素相乘。此外，ndarray的<code>@</code>表示矩阵相乘。</p><p>scipy.org推荐使用ndarray，并且ndarray的应用更为广泛，众多方法的返回值也都是返回ndarray。</p><h2 id="4-2-互相转化"><a href="#4-2-互相转化" class="headerlink" title="4.2 互相转化"></a>4.2 互相转化</h2><p>ndarray通过<code>asmatrix()</code>转成matrix，matrix通过<code>A</code>属性或<code>asarray()</code>转成ndarray。</p><p>需要注意的是，以上转化方法都是浅拷贝。</p><h1 id="5-numpy相关IO"><a href="#5-numpy相关IO" class="headerlink" title="5 numpy相关IO"></a>5 numpy相关IO</h1><h2 id="5-1-加载和保存ndarray"><a href="#5-1-加载和保存ndarray" class="headerlink" title="5.1 加载和保存ndarray"></a>5.1 加载和保存ndarray</h2><p>numpy提供了一系列方法来加载和保存ndarray对象：</p><ul><li><code>np.savetxt()</code>将ndarray保存到文本文件，如<code>.txt</code>或<code>.csv</code></li><li><code>np.loadtxt()</code>从文本文件加载ndarray</li><li><code>np.genfromtxt()</code>也是从文本文件中加载数据，并且可以对缺失值进行处理，相比于loadtxt更慢，但是功能更完备</li><li><code>np.save()</code>将ndarray保存到<code>.npy</code>文件</li><li><code>np.load()</code>从<code>.npy</code>，<code>.npz</code>或者<code>pickle</code>文件加载ndarray</li><li><code>np.savez()</code>将ndarray保存到<code>.npz</code>文件</li></ul><p><code>.npy</code>和<code>.npz</code>文件除了保存原始数据，还会保存shape和dtype等信息，以确保能够完整重构ndarray。</p><h2 id="5-2-与其他类型的数据进行转换"><a href="#5-2-与其他类型的数据进行转换" class="headerlink" title="5.2 与其他类型的数据进行转换"></a>5.2 与其他类型的数据进行转换</h2><h3 id="5-2-1-与python的list互转"><a href="#5-2-1-与python的list互转" class="headerlink" title="5.2.1 与python的list互转"></a>5.2.1 与python的list互转</h3><p><code>np.array()</code>将一个list转成ndarray，<code>tolist()</code>将一个ndarray转成list。</p><h3 id="5-2-2-与pandas的DataFrame和Series互转"><a href="#5-2-2-与pandas的DataFrame和Series互转" class="headerlink" title="5.2.2 与pandas的DataFrame和Series互转"></a>5.2.2 与pandas的DataFrame和Series互转</h3><p>在pandas中，一个Series实际上是个一维数组，而一个DataFrame实际上是个二维数组，但是它们都可以存储异构的数据。</p><p>Series可以通过<code>as_matrix()</code>转换成ndarray。DataFrame的方式比较多，可以通过<code>as_matrix()</code>、<code>values</code>或者<code>np.array()</code>将其转换成ndarray。</p><p>而ndarray则可以分别通过Series和DataFrame的构造函数转换成这两种数据类型。</p><h3 id="5-2-3-与scipy的稀疏矩阵互转"><a href="#5-2-3-与scipy的稀疏矩阵互转" class="headerlink" title="5.2.3 与scipy的稀疏矩阵互转"></a>5.2.3 与scipy的稀疏矩阵互转</h3><p><code>scipy.sparse</code>提供了多种稀疏矩阵，如<code>coo_matrix</code>和<code>csr_matrix</code>，它们可以通过<code>toarray()</code>方法或属性<code>A</code>转换成ndarray，也可以通过<code>todense()</code>转成matrix。</p><p>ndarray可以通过稀疏矩阵的构造函数直接构造稀疏矩阵。</p><h3 id="5-2-4-与pytorch的tensor互转"><a href="#5-2-4-与pytorch的tensor互转" class="headerlink" title="5.2.4 与pytorch的tensor互转"></a>5.2.4 与pytorch的tensor互转</h3><p>pytorch中的tensor可通过<code>torch.from_numpy()</code>，根据ndarray构造tensor。</p><p>tensor转ndarray稍微有点复杂，最简单的情况是<code>numpy()</code>将一个tensor转成ndarray。不过存在两种例外：1）如果一个tensor需要grad，则需要先调用<code>detach()</code>将该tensor分离出计算图，再执行<code>numpy()</code>；2）如果一个tensor在GPU上，则需要先调用<code>cpu()</code>将该tensor移动到cpu上，再执行<code>numpy()</code>。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://numpy.org/">numpy官方文档</a></p><p><a href="https://numpy123.com/">numpy中文文档</a></p><p><a href="https://docs.scipy.org/doc/numpy-1.17.0/reference/arrays.html">scipy中的numpy文档</a></p>]]></content>
    
    
    <categories>
      
      <category>data process</category>
      
    </categories>
    
    
    <tags>
      
      <tag>data process</tag>
      
      <tag>numpy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>markdown踩坑</title>
    <link href="/posts/2730100556/"/>
    <url>/posts/2730100556/</url>
    
    <content type="html"><![CDATA[<p><a href="https://www.npmjs.com/package/hexo-toc">hexo文章内插入目录</a></p>]]></content>
    
    
    <categories>
      
      <category>markdown</category>
      
    </categories>
    
    
    <tags>
      
      <tag>markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo踩坑</title>
    <link href="/posts/856201035/"/>
    <url>/posts/856201035/</url>
    
    <content type="html"><![CDATA[<!-- toc --><p>不得不说，用hexo与github创建个人博客真的是好上手。不过在实际建立博客以及修改主题的时候也遇到了不少的坑，不如第一篇文章就从记录hexo开始吧！</p><h1 id="1-hexo前置配置"><a href="#1-hexo前置配置" class="headerlink" title="1 hexo前置配置"></a>1 hexo前置配置</h1><h2 id="1-1-Node-js与Git"><a href="#1-1-Node-js与Git" class="headerlink" title="1.1 Node.js与Git"></a>1.1 Node.js与Git</h2><p>安装hexo之前需要先安装Node.js和Git，然后可以使用node.js的包管理工具npm来安装hexo：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">npm install -g hexo-cli</span><br></code></pre></td></tr></table></figure><p>具体Node.js的版本，可以参考所将要使用的主题配置，不同主题对于Node.js和npm的版本要求不一样。</p><h2 id="1-2-与github建立认证"><a href="#1-2-与github建立认证" class="headerlink" title="1.2 与github建立认证"></a>1.2 与github建立认证</h2><p>为避免每次部署网站的时候，都需要进行认证，需要提前与github建立认证关系，可以有以下两种方式。</p><h3 id="1-2-1-SSH方式"><a href="#1-2-1-SSH方式" class="headerlink" title="1.2.1 SSH方式"></a>1.2.1 SSH方式</h3><p>首先创建公钥和密钥，在命令行中输入如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">ssh-keygen -t rsa -C <span class="hljs-string">&quot;email@email.com&quot;</span></span><br></code></pre></td></tr></table></figure><p>其中<code>&quot;email@email.com&quot;</code>为github绑定的邮箱。之后会生成密钥<code>id_rsa</code>和公钥<code>id_rsa.pub</code>。</p><p>然后在github中进行配置：在github右上角头像中打开setttings，在侧边栏找到<code>SSH and GPG keys</code>，然后点击<code>New SSH key</code>，将公钥中的内容复制到文本框Key中即可。</p><h3 id="1-2-2-Personal-access-token方式"><a href="#1-2-2-Personal-access-token方式" class="headerlink" title="1.2.2 Personal access token方式"></a>1.2.2 Personal access token方式</h3><p>Github已于2021.08.13停止使用密码验证，可以通过Personal access Token的方式实现短期内的认证。</p><p>在github右上角头像中打开setttings，在侧边栏找到<code>Developer settings</code>，然后点击<code>Personal access tokens</code>，新建一个token。一般给自己用的话，所有的scope都选上即可。然后复制生成的token，并修改博客目录下的配置文件：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-attr">deploy:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">git</span><br>  <span class="hljs-attr">repo:</span> <span class="hljs-string">https://[token]@github.com/[username]/[repo].git</span><br>  <span class="hljs-attr">branch:</span> [<span class="hljs-string">branch</span>]<br></code></pre></td></tr></table></figure><h1 id="2-hexo基本使用"><a href="#2-hexo基本使用" class="headerlink" title="2 hexo基本使用"></a>2 hexo基本使用</h1><p>hexo命令行以<code>hexo</code>开始，然后输入关键词，基本上所有的关键词都可以被缩写其首字母的形式。</p><h2 id="2-1-hexo建站"><a href="#2-1-hexo建站" class="headerlink" title="2.1 hexo建站"></a>2.1 hexo建站</h2><p>首先通过<code>init</code>命令创建hexo网站文件夹，然后通过<code>npm</code>安装必备文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">hexo init &lt;folder&gt;</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">cd</span> &lt;folder&gt;</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">npm install</span><br></code></pre></td></tr></table></figure><p><code>&lt;folder&gt;</code>必须是个空文件夹，否则会报错。</p><p>建站后，需要对博客目录下的<code>_config.yml</code>中的<code>deploy</code>项进行配置，与博客网站内容对齐。</p><h2 id="2-2-网站发布"><a href="#2-2-网站发布" class="headerlink" title="2.2 网站发布"></a>2.2 网站发布</h2><h3 id="2-2-1-清除缓存"><a href="#2-2-1-清除缓存" class="headerlink" title="2.2.1 清除缓存"></a>2.2.1 清除缓存</h3><p>有时候为了避免不必要的错误，在生成静态文件之前，可以先运行如下命令清除缓存：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">hexo clean</span><br></code></pre></td></tr></table></figure><p>该命令会清除本地站点文件夹下的缓存文件(<code>db.json</code>)和已有的静态文件夹(<code>public</code>)。</p><h3 id="2-2-2-生成静态页面"><a href="#2-2-2-生成静态页面" class="headerlink" title="2.2.2 生成静态页面"></a>2.2.2 生成静态页面</h3><p>执行如下命令可以生成静态网页，静态网页及相关资源均在public目录下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">hexo generate</span><br></code></pre></td></tr></table></figure><h3 id="2-2-3-部署到Git"><a href="#2-2-3-部署到Git" class="headerlink" title="2.2.3 部署到Git"></a>2.2.3 部署到Git</h3><p>生成好静态网页之后，运行如下命令部署到GitHub上：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">hexo deploy</span><br></code></pre></td></tr></table></figure><p>上述几个命令可以通过<code>&amp;&amp;</code>合在一起执行，如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy</span><br></code></pre></td></tr></table></figure><h3 id="2-2-4-启动本地服务器"><a href="#2-2-4-启动本地服务器" class="headerlink" title="2.2.4 启动本地服务器"></a>2.2.4 启动本地服务器</h3><p>如果想要在本地查看网页效果，可以启动服务器，然后在<code>http://localhost:4000</code>可以查看网页。服务器启动期间，hexo会监视文件变动并自动更新。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">hexo server</span><br></code></pre></td></tr></table></figure><p>如果想更改端口，或者端口被占用，可以使用<code>-p</code>指定端口，如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">hexo server -p 5000</span><br></code></pre></td></tr></table></figure><h3 id="2-2-5-查看版本"><a href="#2-2-5-查看版本" class="headerlink" title="2.2.5 查看版本"></a>2.2.5 查看版本</h3><p>查看hexo版本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">hexo version</span><br></code></pre></td></tr></table></figure><h2 id="2-3-创建新文章或页面"><a href="#2-3-创建新文章或页面" class="headerlink" title="2.3 创建新文章或页面"></a>2.3 创建新文章或页面</h2><p>使用以下命令创建新文章或者新页面：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">hexo new [layout] &lt;title&gt;</span><br></code></pre></td></tr></table></figure><p>layout指定了布局类型，默认是文章(post)，然后会在<code>source/_posts</code>下创建一个md文件。</p><h2 id="2-4-Layout类型"><a href="#2-4-Layout类型" class="headerlink" title="2.4 Layout类型"></a>2.4 Layout类型</h2><p>Hexo包含三种Layout：<code>post</code>、<code>page</code>和<code>draft</code>，默认布局可以在<code>_config.yml</code>文件中修改。</p><p><code>post</code>即博客中的各种文章，如本踩坑指北，所有页面存在<code>source/_posts</code>下；<br><code>page</code>则包含了首页、分类、标签页等页面，所有页面存在<code>source</code>下，archives默认由<code>hexo g</code>生成；<br><code>draft</code>即草稿，位于<code>source/_drafts</code>下，可通过<code>publish</code>命令发布：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">hexo publish [layout] &lt;title&gt;</span><br></code></pre></td></tr></table></figure><h2 id="2-5-hexo文件夹目录"><a href="#2-5-hexo文件夹目录" class="headerlink" title="2.5 hexo文件夹目录"></a>2.5 hexo文件夹目录</h2><h3 id="2-5-1-node-module"><a href="#2-5-1-node-module" class="headerlink" title="2.5.1 node_module"></a>2.5.1 node_module</h3><p>存放npm与hexo等与Node.js有关内容的文件夹，<code>npm install</code>所安装的库默认存在该文件夹下。</p><h3 id="2-5-2-public"><a href="#2-5-2-public" class="headerlink" title="2.5.2 public"></a>2.5.2 public</h3><p>存放生成的静态网页和相关资源的文件夹，与github中的内容保持对齐。</p><h3 id="2-5-3-scaffolds"><a href="#2-5-3-scaffolds" class="headerlink" title="2.5.3 scaffolds"></a>2.5.3 scaffolds</h3><p>存放模板配置的文件夹，新建文章时，根据存放的模板来建立文件。</p><h3 id="2-5-4-source"><a href="#2-5-4-source" class="headerlink" title="2.5.4 source"></a>2.5.4 source</h3><p>存放用户资源的文件夹。该文件夹下预先只含有<code>_post</code>文件夹，分类页面、标签页面、图片等需要自己手动创建文件夹。除了<code>_post</code>文件夹，其他开头为<code>_</code>的文件或文件夹、隐藏文件将被忽略。</p><h3 id="2-5-5-themes"><a href="#2-5-5-themes" class="headerlink" title="2.5.5 themes"></a>2.5.5 themes</h3><p>存放有第三方和自定义的主题样式的文件夹。</p><h3 id="2-5-6-config-yml"><a href="#2-5-6-config-yml" class="headerlink" title="2.5.6 _config.yml"></a>2.5.6 _config.yml</h3><p>博客网站配置文件，在此配置大部分参数。在使用第三方主题的时候，一般会多一个主题配置文件，大部分主题下两个配置文件不冲突。</p><h1 id="3-hexo常用配置事项（待更新中）"><a href="#3-hexo常用配置事项（待更新中）" class="headerlink" title="3 hexo常用配置事项（待更新中）"></a>3 hexo常用配置事项（待更新中）</h1><h2 id="3-1-添加常用页面"><a href="#3-1-添加常用页面" class="headerlink" title="3.1 添加常用页面"></a>3.1 添加常用页面</h2><p>根据主题的<code>_config.yml</code>，找到<code>menu</code>字段，查看其下各常用页面的目录结构，然后<code>hexo new page [name]</code>创建对应页面。一般情况下，“关于我”为“about”，标签为“tags”，分类为“categories”，归档为“archives”，不过具体情况依各主题模板开发情况而定。也可自定义新的页面，然后配置<code>_config.yml</code>文件。</p><h2 id="3-2-修改文章链接为随机编号"><a href="#3-2-修改文章链接为随机编号" class="headerlink" title="3.2 修改文章链接为随机编号"></a>3.2 修改文章链接为随机编号</h2><p>有些时候，创建的post名字中会带有中文，并且Hexo默认一个post的网址为<code>year/:month/:day/:title/</code>，这种情况下链接可能会非常长，并且如果作者修改了标题或者日期，那么链接立马就会失效，这会带来不便。<code>abbrlink</code>插件能够通过将链接改为随机编号的方式，帮助解决这个问题。</p><p>首先在博客根目录下，安装abbrlink插件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">npm install hexo-abbrlink --save</span><br></code></pre></td></tr></table></figure><p>然后打开博客根目录下的<code>_config.yml</code>文件，修改配置：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">permalink:</span> posts/:abbrlink/<br><span class="hljs-symbol">abbrlink:</span><br><span class="hljs-symbol">  alg:</span> crc32 <span class="hljs-meta">#support crc16(default) and crc32</span><br><span class="hljs-symbol">  rep:</span> dec   <span class="hljs-meta">#support dec(default) and hex</span><br></code></pre></td></tr></table></figure><p>alg指定使用的算法，支持crc16和crc32两种，默认crc16。rep指定使用的进制，dec表示十进制，hex表示十六进制，默认dec。</p><p>配置完成后，重新使用<code>hexo clean &amp;&amp; hexo g</code>生成博客。之后便可以在post的front-matter处，看到abbrlink字段。</p><p>不过该插件有可能因为版本冲突引发漏洞，可以通过<code>npm install hexo-abbrlink2 --save</code>安装另一个版本以避免冲突。</p><h2 id="3-3-本地服务自动刷新"><a href="#3-3-本地服务自动刷新" class="headerlink" title="3.3 本地服务自动刷新"></a>3.3 本地服务自动刷新</h2><p>一般想快速查看结果的时候，会使用<code>hexo s</code>开启本地服务器。但是在编写markdown的过程中，经常会碰到不断修改调试的情况，但是即使是<code>hexo s</code>，也做不到时时更新，需要借助<code>browsersyns</code>插件补足这个功能。<br>首先在任意目录下执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">npm install -g browser-sync<br></code></pre></td></tr></table></figure><p>然后在博客根目录下安装插件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">npm install hexo-browsersync --save<br></code></pre></td></tr></table></figure><p>之后通过<code>hexo s</code>启动本地服务器，每次保存markdown文件，都会本地服务器都会实时显示更新。</p><h2 id="3-4-hexo主题更换"><a href="#3-4-hexo主题更换" class="headerlink" title="3.4 hexo主题更换"></a>3.4 hexo主题更换</h2><p>大部分hexo主题的更换在操作上并不复杂，只需要git clone相应的主题文件夹，然后放入themes文件夹中，接着修改<code>_config.yml</code>中的<code>theme</code>的值即可使用，相关配置可见各主题的文档。但是在诸如<code>npm install</code>及各种配置地方，极易遇到各种问题与Bug。建议使用有详细文档说明的主题，调试起来比较方便。</p><p>我所比较喜欢的主题有：Butterfly，Fluid，huxblog，diaspora，Keep，Clean，BeanTech。目前使用的Fluid，其文档见<a href="https://fluid-dev.github.io/hexo-fluid-docs/guide/">这里</a>。</p><h2 id="3-5-hexo文件夹迁移"><a href="#3-5-hexo文件夹迁移" class="headerlink" title="3.5 hexo文件夹迁移"></a>3.5 hexo文件夹迁移</h2><p>hexo文件夹在迁移时，可以直接将原来的hexo文件夹整个复制到新的博客文件夹下，移动前目标文件夹需要通过命令<code>hexo init &lt;folder&gt;</code>进行过初始化。</p><h1 id="4-坑点记录"><a href="#4-坑点记录" class="headerlink" title="4 坑点记录"></a>4 坑点记录</h1><ol><li>初次使用hexo的时候，可能会搞不清楚资源存放在哪个目录下。source文件夹下默认只有<code>_posts</code>文件夹用于存放各个post页面，但是诸如其他page以及图片文件夹需要自己手动创建，比如存放在source&#x2F;img文件夹下的图片才会被<code>hexo g</code>生成到<code>public/img</code>文件夹下面。如果图片等资源直接放在<code>public</code>文件夹下，则每次<code>hexo clean</code>的时候会被清除掉。</li><li>建议在<code>hexo s</code>启动localhost的情况下进行调试，能够在本地快速地查看结果。<code>hexo g &amp;&amp; hexo d</code>最好是用来进行网页的实际更新，它存在三个问题：1）命令执行起来比较耗时；2）网页更新响应比较慢，不如<code>hexo s</code>基本可以在本地实时查看变动；3）如果不是在无痕模式下浏览，则之前访问时留下的缓存数据会影响后续网页更新。因此每次在执行命令后需要清除缓存，隔一段时间才能看到更新后的网页。</li><li><code>npm install</code>时经常会碰到安装失败的问题，需要反复试验是否有设置镜像或者梯子。</li><li>如果更换主题后，显示错误<code>extends includes/layout.pug block content include ./includes/mixins/post-ui.pug #recent-posts.recent-posts +postUI include includes/pagination.pug</code>，请见该<a href="https://www.jianshu.com/p/aa936a8369fb">博客</a>。</li><li>博客目录下的配置文件中，需要配置git。如果在<code>hexo d</code>的过程中卡住，则大概率需要配置git代理，以加速与github的连接。如果显示authentication失败，并提示<code>Please use a personal access token instead.</code>，则需要配置github的<code>Personal access tokens</code>，github已于2021.08.13停止使用密码验证git。</li><li>如果<code>hexo d</code>报错<code>ERROR Deployer not found: git</code>，则需在博客目录下安装相应插件<code>npm install hexo-deployer-git --save</code>。</li><li>如果在<code>hexo init &lt;folder&gt;</code>或者<code>hexo d</code>等涉及到git的过程中，遇到类似如下类型的错误，则是网络问题导致的，需要进行网络故障排查，或设置github代理：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs ERROR">FATAL &#123;<br>  err: Error: Spawn failed<br>      at ChildProcess.&lt;anonymous&gt; (E:\blog\node_modules\hexo-util\lib\spawn.js:51:21)<br>      at ChildProcess.emit (node:events:390:28)<br>      at ChildProcess.cp.emit (E:\blog\node_modules\cross-spawn\lib\enoent.js:34:29)<br>      at Process.ChildProcess._handle.onexit (node:internal/child_process:290:12) &#123;<br>    code: 128<br>  &#125;<br>&#125; Something&#x27;s wrong. Maybe you can find the solution here: %s https://hexo.io/docs/troubleshooting.html<br></code></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <categories>
      
      <category>hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
